<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>TENER</title>
    <url>/2022/02/21/TENER/</url>
    <content><![CDATA[<p><img src="/2022/02/21/TENER/workflow.png" alt="TENER"></p>
<span id="more"></span>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><p>encoding: utf-8<br>import torch<br>from torch import nn<br>import torch.nn.functional as F<br>from fastNLP.modules import ConditionalRandomField, allowed_transitions<br>from modules.transformer import TransformerEncoder</p>
<p>class TENER(nn.Module):<br>    def <strong>init</strong>(self, tag_vocab, embed, num_layers, d_model, n_head, feedforward_dim, dropout,<br>                 after_norm=True, attn_type=’adatrans’, bi_embed=None,<br>                 fc_dropout=0.3, pos_embed=None, scale=False, dropout_attn=None):<br>        “””</p>
<pre><code>    :param tag_vocab: fastNLP Vocabulary
    :param embed: fastNLP TokenEmbedding
    :param num_layers: number of self-attention layers
    :param d_model: input size
    :param n_head: number of head
    :param feedforward_dim: the dimension of ffn
    :param dropout: dropout in self-attention
    :param after_norm: normalization place
    :param attn_type: adatrans, naive
    :param rel_pos_embed: position embedding的类型，支持sin, fix, None. relative时可为None
    :param bi_embed: Used in Chinese scenerio
    :param fc_dropout: dropout rate before the fc layer
    &quot;&quot;&quot;
    super().__init__()

    self.embed = embed
    embed_size = self.embed.embed_size
    self.bi_embed = None
    if bi_embed is not None:
        self.bi_embed = bi_embed
        embed_size += self.bi_embed.embed_size

    self.in_fc = nn.Linear(embed_size, d_model)

    self.transformer = TransformerEncoder(num_layers, d_model, n_head, feedforward_dim, dropout,
                                          after_norm=after_norm, attn_type=attn_type,
                                          scale=scale, dropout_attn=dropout_attn,
                                          pos_embed=pos_embed)
    self.fc_dropout = nn.Dropout(fc_dropout)
    self.out_fc = nn.Linear(d_model, len(tag_vocab))

    trans = allowed_transitions(tag_vocab, include_start_end=True)
    self.crf = ConditionalRandomField(len(tag_vocab), include_start_end_trans=True, allowed_transitions=trans)

def _forward(self, chars, target, bigrams=None):
    mask = chars.ne(0)
    chars = self.embed(chars)
    if self.bi_embed is not None:
        bigrams = self.bi_embed(bigrams)
        chars = torch.cat([chars, bigrams], dim=-1)

    chars = self.in_fc(chars)
    chars = self.transformer(chars, mask)
    chars = self.fc_dropout(chars)
    chars = self.out_fc(chars)
    logits = F.log_softmax(chars, dim=-1)
    if target is None:
        paths, _ = self.crf.viterbi_decode(logits, mask)
        return &#123;&#39;pred&#39;: paths&#125;
    else:
        loss = self.crf(logits, target, mask)
        return &#123;&#39;loss&#39;: loss&#125;

def forward(self, chars, target, bigrams=None):
    return self._forward(chars, target, bigrams)

def predict(self, chars, bigrams=None):
    return self._forward(chars, target=None, bigrams=bigrams)
</code></pre>
]]></content>
      <categories>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/02/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>平时做的一些东西</title>
    <url>/2022/02/21/work-collection/</url>
    <content><![CDATA[<h1 id="本科的作品集"><a href="#本科的作品集" class="headerlink" title="本科的作品集"></a>本科的作品集</h1><span id="more"></span>
<h2 id="电影app的UI设计"><a href="#电影app的UI设计" class="headerlink" title="电影app的UI设计"></a>电影app的UI设计</h2><p><img src="/2022/02/21/work-collection/ui%E8%AE%BE%E8%AE%A1.jpg" alt="ui设计"></p>
<h2 id="3DMAX"><a href="#3DMAX" class="headerlink" title="3DMAX"></a>3DMAX</h2><p><img src="/2022/02/21/work-collection/3dmax1.jpg" alt="3DMAX"><br><img src="/2022/02/21/work-collection/3dmax2.jpg" alt="3DMAX"></p>
<h2 id="unity3D"><a href="#unity3D" class="headerlink" title="unity3D"></a>unity3D</h2><p><img src="/2022/02/21/work-collection/unity3D.jpg" alt="unity3d"></p>
<h2 id="AR制作"><a href="#AR制作" class="headerlink" title="AR制作"></a>AR制作</h2><p><img src="/2022/02/21/work-collection/AR.jpg" alt="AR"></p>
<h2 id="全景图"><a href="#全景图" class="headerlink" title="全景图"></a>全景图</h2><p><img src="/2022/02/21/work-collection/%E5%85%A8%E6%99%AF%E5%9B%BE.jpg" alt="全景图"></p>
]]></content>
      <categories>
        <category>作品集</category>
      </categories>
      <tags>
        <tag>-作品</tag>
      </tags>
  </entry>
  <entry>
    <title>初次见面</title>
    <url>/2022/02/17/%E5%88%9D%E6%AC%A1%E8%A7%81%E9%9D%A2/</url>
    <content><![CDATA[<h1 id="第一篇Blog"><a href="#第一篇Blog" class="headerlink" title="第一篇Blog"></a>第一篇Blog</h1><h2 id="初次见面，之后请多多关照"><a href="#初次见面，之后请多多关照" class="headerlink" title="初次见面，之后请多多关照"></a>初次见面，之后请多多关照</h2><span id="more"></span>
<p>这个博客简单记录一下学习以及平时的一些碎碎念</p>
]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>碎碎念</tag>
      </tags>
  </entry>
  <entry>
    <title>BiLSTM-CRF</title>
    <url>/2022/02/20/BiLSTM-CRF/</url>
    <content><![CDATA[<p><img src="/2022/02/20/BiLSTM-CRF/bilstm.jpg" alt="原理图"></p>
<span id="more"></span>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><p>‘’’<br>    # -<em>- coding: utf-8 -</em>-<br>from <strong>future</strong> import print_function<br>import time<br>import sys<br>import argparse<br>import random<br>import torch<br>import gc<br>import torch.nn as nn<br>import torch.optim as optim<br>import numpy as np<br>from utils.metric import get_ner_fmeasure<br>from model.seqlabel import SeqLabel<br>from model.sentclassifier import SentClassifier<br>from utils.data import Data</p>
<p>try:<br>    import cPickle as pickle<br>except ImportError:<br>    import pickle</p>
<p>seed_num = 42<br>random.seed(seed_num)<br>torch.manual_seed(seed_num)<br>np.random.seed(seed_num)</p>
<p>def data_initialization(data):<br>    data.initial_feature_alphabets()<br>    data.build_alphabet(data.train_dir)<br>    data.build_alphabet(data.dev_dir)<br>    data.build_alphabet(data.test_dir)<br>    data.fix_alphabet()</p>
<p>def predict_check(pred_variable, gold_variable, mask_variable, sentence_classification=False):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result, in numpy format<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””<br>    pred = pred_variable.cpu().data.numpy()<br>    gold = gold_variable.cpu().data.numpy()<br>    mask = mask_variable.cpu().data.numpy()<br>    overlaped = (pred == gold)<br>    if sentence_classification:<br>        # print(overlaped)<br>        # print(overlaped*pred)<br>        right_token = np.sum(overlaped)<br>        total_token = overlaped.shape[0] ## =batch_size<br>    else:<br>        right_token = np.sum(overlaped * mask)<br>        total_token = mask.sum()<br>    # print(“right: %s, total: %s”%(right_token, total_token))<br>    return right_token, total_token</p>
<p>def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover, sentence_classification=False):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””<br>    pred_variable = pred_variable[word_recover]<br>    gold_variable = gold_variable[word_recover]<br>    mask_variable = mask_variable[word_recover]<br>    batch_size = gold_variable.size(0)<br>    if sentence_classification:<br>        pred_tag = pred_variable.cpu().data.numpy().tolist()<br>        gold_tag = gold_variable.cpu().data.numpy().tolist()<br>        pred_label = [label_alphabet.get_instance(pred) for pred in pred_tag]<br>        gold_label = [label_alphabet.get_instance(gold) for gold in gold_tag]<br>    else:<br>        seq_len = gold_variable.size(1)<br>        mask = mask_variable.cpu().data.numpy()<br>        pred_tag = pred_variable.cpu().data.numpy()<br>        gold_tag = gold_variable.cpu().data.numpy()<br>        batch_size = mask.shape[0]<br>        pred_label = []<br>        gold_label = []<br>        for idx in range(batch_size):<br>            pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]<br>            gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]<br>            assert(len(pred)==len(gold))<br>            pred_label.append(pred)<br>            gold_label.append(gold)<br>    return pred_label, gold_label</p>
<p>def recover_nbest_label(pred_variable, mask_variable, label_alphabet, word_recover):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len, nbest): pred tag result<br>            mask_variable (batch_size, sent_len): mask variable<br>            word_recover (batch_size)<br>        output:<br>            nbest_pred_label list: [batch_size, nbest, each_seq_len]<br>    “””<br>    # exit(0)<br>    pred_variable = pred_variable[word_recover]<br>    mask_variable = mask_variable[word_recover]<br>    batch_size = pred_variable.size(0)<br>    seq_len = pred_variable.size(1)<br>    nbest = pred_variable.size(2)<br>    mask = mask_variable.cpu().data.numpy()<br>    pred_tag = pred_variable.cpu().data.numpy()<br>    batch_size = mask.shape[0]<br>    pred_label = []<br>    for idx in range(batch_size):<br>        pred = []<br>        for idz in range(nbest):<br>            each_pred = [label_alphabet.get_instance(pred_tag[idx][idy][idz]) for idy in range(seq_len) if mask[idx][idy] != 0]<br>            pred.append(each_pred)<br>        pred_label.append(pred)<br>    return pred_label</p>
<p>def lr_decay(optimizer, epoch, decay_rate, init_lr):<br>    lr = init_lr/(1+decay_rate*epoch)<br>    print(“ Learning rate is set as:”, lr)<br>    for param_group in optimizer.param_groups:<br>        param_group[‘lr’] = lr<br>    return optimizer</p>
<p>def evaluate(data, model, name, nbest=None):<br>    if name == “train”:<br>        instances = data.train_Ids<br>    elif name == “dev”:<br>        instances = data.dev_Ids<br>    elif name == ‘test’:<br>        instances = data.test_Ids<br>    elif name == ‘raw’:<br>        instances = data.raw_Ids<br>    else:<br>        print(“Error: wrong evaluate name,”, name)<br>        exit(1)<br>    right_token = 0<br>    whole_token = 0<br>    nbest_pred_results = []<br>    pred_scores = []<br>    pred_results = []<br>    gold_results = []<br>    ## set model in eval model<br>    model.eval()<br>    batch_size = data.HP_batch_size<br>    start_time = time.time()<br>    train_num = len(instances)<br>    total_batch = train_num//batch_size+1<br>    for batch_id in range(total_batch):<br>        start = batch_id*batch_size<br>        end = (batch_id+1)*batch_size<br>        if end &gt; train_num:<br>            end =  train_num<br>        instance = instances[start:end]<br>        if not instance:<br>            continue<br>        batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, False, data.sentence_classification)<br>        if nbest and not data.sentence_classification:<br>            scores, nbest_tag_seq = model.decode_nbest(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask, nbest)<br>            nbest_pred_result = recover_nbest_label(nbest_tag_seq, mask, data.label_alphabet, batch_wordrecover)<br>            nbest_pred_results += nbest_pred_result<br>            pred_scores += scores[batch_wordrecover].cpu().data.numpy().tolist()<br>            ## select the best sequence to evalurate<br>            tag_seq = nbest_tag_seq[:,:,0]<br>        else:<br>            tag_seq = model(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)<br>        # print(“tag:”,tag_seq)<br>        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover, data.sentence_classification)<br>        pred_results += pred_label<br>        gold_results += gold_label<br>    decode_time = time.time() - start_time<br>    speed = len(instances)/decode_time<br>    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)<br>    if nbest and not data.sentence_classification:<br>        return speed, acc, p, r, f, nbest_pred_results, pred_scores<br>    return speed, acc, p, r, f, pred_results, pred_scores</p>
<p>def batchify_with_label(input_batch_list, gpu, if_train=True, sentence_classification=False):<br>    if sentence_classification:<br>        return batchify_sentence_classification_with_label(input_batch_list, gpu, if_train)<br>    else:<br>        return batchify_sequence_labeling_with_label(input_batch_list, gpu, if_train)</p>
<p>def batchify_sequence_labeling_with_label(input_batch_list, gpu, if_train=True):<br>    “””<br>        input: list of words, chars and labels, various length. [[words, features, chars, labels],[words, features, chars,labels],…]<br>            words: word ids for one sentence. (batch_size, sent_len)<br>            features: features ids for one sentence. (batch_size, sent_len, feature_num)<br>            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)<br>            labels: label ids for one sentence. (batch_size, sent_len)</p>
<pre><code>    output:
        zero padding for word and char, with their batch length
        word_seq_tensor: (batch_size, max_sent_len) Variable
        feature_seq_tensors: [(batch_size, max_sent_len),...] list of Variable
        word_seq_lengths: (batch_size,1) Tensor
        char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable
        char_seq_lengths: (batch_size*max_sent_len,1) Tensor
        char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order
        label_seq_tensor: (batch_size, max_sent_len)
        mask: (batch_size, max_sent_len)
&quot;&quot;&quot;
batch_size = len(input_batch_list)
words = [sent[0] for sent in input_batch_list]
features = [np.asarray(sent[1]) for sent in input_batch_list]
feature_num = len(features[0][0])
chars = [sent[2] for sent in input_batch_list]
labels = [sent[3] for sent in input_batch_list]
word_seq_lengths = torch.LongTensor(list(map(len, words)))
max_seq_len = word_seq_lengths.max().item()
word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).long()
label_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).long()
feature_seq_tensors = []
for idx in range(feature_num):
    feature_seq_tensors.append(torch.zeros((batch_size, max_seq_len),requires_grad =  if_train).long())
mask = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).bool()
for idx, (seq, label, seqlen) in enumerate(zip(words, labels, word_seq_lengths)):
    seqlen = seqlen.item()
    word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)
    label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)
    mask[idx, :seqlen] = torch.Tensor([1]*seqlen)
    for idy in range(feature_num):
        feature_seq_tensors[idy][idx,:seqlen] = torch.LongTensor(features[idx][:,idy])
word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)
word_seq_tensor = word_seq_tensor[word_perm_idx]
for idx in range(feature_num):
    feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]

label_seq_tensor = label_seq_tensor[word_perm_idx]
mask = mask[word_perm_idx]
### deal with char
# pad_chars (batch_size, max_seq_len)
pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]
length_list = [list(map(len, pad_char)) for pad_char in pad_chars]
max_word_len = max(map(max, length_list))
char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len), requires_grad =  if_train).long()
char_seq_lengths = torch.LongTensor(length_list)
for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):
    for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):
        # print len(word), wordlen
        char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)

char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)
char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)
char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)
char_seq_tensor = char_seq_tensor[char_perm_idx]
_, char_seq_recover = char_perm_idx.sort(0, descending=False)
_, word_seq_recover = word_perm_idx.sort(0, descending=False)
if gpu:
    word_seq_tensor = word_seq_tensor.cuda()
    for idx in range(feature_num):
        feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()
    word_seq_lengths = word_seq_lengths.cuda()
    word_seq_recover = word_seq_recover.cuda()
    label_seq_tensor = label_seq_tensor.cuda()
    char_seq_tensor = char_seq_tensor.cuda()
    char_seq_recover = char_seq_recover.cuda()
    mask = mask.cuda()
return word_seq_tensor,feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask
</code></pre>
<p>def batchify_sentence_classification_with_label(input_batch_list, gpu, if_train=True):<br>    “””<br>        input: list of words, chars and labels, various length. [[words, features, chars, labels],[words, features, chars,labels],…]<br>            words: word ids for one sentence. (batch_size, sent_len)<br>            features: features ids for one sentence. (batch_size, feature_num), each sentence has one set of feature<br>            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)<br>            labels: label ids for one sentence. (batch_size,), each sentence has one set of feature</p>
<pre><code>    output:
        zero padding for word and char, with their batch length
        word_seq_tensor: (batch_size, max_sent_len) Variable
        feature_seq_tensors: [(batch_size,), ... ] list of Variable
        word_seq_lengths: (batch_size,1) Tensor
        char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable
        char_seq_lengths: (batch_size*max_sent_len,1) Tensor
        char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order
        label_seq_tensor: (batch_size, )
        mask: (batch_size, max_sent_len)
&quot;&quot;&quot;

batch_size = len(input_batch_list)
words = [sent[0] for sent in input_batch_list]
features = [np.asarray(sent[1]) for sent in input_batch_list]    
feature_num = len(features[0])
chars = [sent[2] for sent in input_batch_list]
labels = [sent[3] for sent in input_batch_list]
word_seq_lengths = torch.LongTensor(list(map(len, words)))
max_seq_len = word_seq_lengths.max().item()
word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).long()
label_seq_tensor = torch.zeros((batch_size, ), requires_grad =  if_train).long()
feature_seq_tensors = []
for idx in range(feature_num):
    feature_seq_tensors.append(torch.zeros((batch_size, max_seq_len),requires_grad =  if_train).long())
mask = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).bool()
label_seq_tensor = torch.LongTensor(labels)
# exit(0)
for idx, (seq,  seqlen) in enumerate(zip(words,  word_seq_lengths)):
    seqlen = seqlen.item()
    word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)
    mask[idx, :seqlen] = torch.Tensor([1]*seqlen)
    for idy in range(feature_num):
        feature_seq_tensors[idy][idx,:seqlen] = torch.LongTensor(features[idx][:,idy])
word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)
word_seq_tensor = word_seq_tensor[word_perm_idx]
for idx in range(feature_num):
    feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]
label_seq_tensor = label_seq_tensor[word_perm_idx]
mask = mask[word_perm_idx]
### deal with char
# pad_chars (batch_size, max_seq_len)
pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]
length_list = [list(map(len, pad_char)) for pad_char in pad_chars]
max_word_len = max(map(max, length_list))
char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len), requires_grad =  if_train).long()
char_seq_lengths = torch.LongTensor(length_list)
for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):
    for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):
        # print len(word), wordlen
        char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)

char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)
char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)
char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)
char_seq_tensor = char_seq_tensor[char_perm_idx]
_, char_seq_recover = char_perm_idx.sort(0, descending=False)
_, word_seq_recover = word_perm_idx.sort(0, descending=False)
if gpu:
    word_seq_tensor = word_seq_tensor.cuda()
    for idx in range(feature_num):
        feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()
    word_seq_lengths = word_seq_lengths.cuda()
    word_seq_recover = word_seq_recover.cuda()
    label_seq_tensor = label_seq_tensor.cuda()
    char_seq_tensor = char_seq_tensor.cuda()
    char_seq_recover = char_seq_recover.cuda()
    mask = mask.cuda()
return word_seq_tensor,feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask
</code></pre>
<p>def train(data):<br>    print(“Training model…”)<br>    data.show_data_summary()<br>    save_data_name = data.model_dir +”.dset”<br>    data.save(save_data_name)<br>    if data.sentence_classification:<br>        model = SentClassifier(data)<br>    else:<br>        model = SeqLabel(data)</p>
<pre><code>if data.optimizer.lower() == &quot;sgd&quot;:
    optimizer = optim.SGD(model.parameters(), lr=data.HP_lr, momentum=data.HP_momentum,weight_decay=data.HP_l2)
elif data.optimizer.lower() == &quot;adagrad&quot;:
    optimizer = optim.Adagrad(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)
elif data.optimizer.lower() == &quot;adadelta&quot;:
    optimizer = optim.Adadelta(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)
elif data.optimizer.lower() == &quot;rmsprop&quot;:
    optimizer = optim.RMSprop(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)
elif data.optimizer.lower() == &quot;adam&quot;:
    optimizer = optim.Adam(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)
else:
    print(&quot;Optimizer illegal: %s&quot;%(data.optimizer))
    exit(1)
best_dev = -10
# data.HP_iteration = 1
## start training
for idx in range(data.HP_iteration):
    epoch_start = time.time()
    temp_start = epoch_start
    print(&quot;Epoch: %s/%s&quot; %(idx,data.HP_iteration))
    if data.optimizer == &quot;SGD&quot;:
        optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)
    instance_count = 0
    sample_id = 0
    sample_loss = 0
    total_loss = 0
    right_token = 0
    whole_token = 0
    random.shuffle(data.train_Ids)
    print(&quot;Shuffle: first input word list:&quot;, data.train_Ids[0][0])
    ## set model in train model
    model.train()
    model.zero_grad()
    batch_size = data.HP_batch_size
    batch_id = 0
    train_num = len(data.train_Ids)
    total_batch = train_num//batch_size+1
    for batch_id in range(total_batch):
        start = batch_id*batch_size
        end = (batch_id+1)*batch_size
        if end &gt;train_num:
            end = train_num
        instance = data.train_Ids[start:end]
        if not instance:
            continue
        batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True, data.sentence_classification)
        instance_count += 1
        loss, tag_seq = model.calculate_loss(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)
        right, whole = predict_check(tag_seq, batch_label, mask, data.sentence_classification)
        right_token += right
        whole_token += whole
        # print(&quot;loss:&quot;,loss.item())
        sample_loss += loss.item()
        total_loss += loss.item()
        if end%500 == 0:
            temp_time = time.time()
            temp_cost = temp_time - temp_start
            temp_start = temp_time
            print(&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot;%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))
            if sample_loss &gt; 1e8 or str(sample_loss) == &quot;nan&quot;:
                print(&quot;ERROR: LOSS EXPLOSION (&gt;1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT....&quot;)
                exit(1)
            sys.stdout.flush()
            sample_loss = 0
        loss.backward()
        optimizer.step()
        model.zero_grad()
    temp_time = time.time()
    temp_cost = temp_time - temp_start
    print(&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot;%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))

    epoch_finish = time.time()
    epoch_cost = epoch_finish - epoch_start
    print(&quot;Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s&quot;%(idx, epoch_cost, train_num/epoch_cost, total_loss))
    print(&quot;totalloss:&quot;, total_loss)
    if total_loss &gt; 1e8 or str(total_loss) == &quot;nan&quot;:
        print(&quot;ERROR: LOSS EXPLOSION (&gt;1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT....&quot;)
        exit(1)
    # continue
    speed, acc, p, r, f, _,_ = evaluate(data, model, &quot;dev&quot;)
    dev_finish = time.time()
    dev_cost = dev_finish - epoch_finish

    if data.seg:
        current_score = f
        print(&quot;Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot;%(dev_cost, speed, acc, p, r, f))
    else:
        current_score = acc
        print(&quot;Dev: time: %.2fs speed: %.2fst/s; acc: %.4f&quot;%(dev_cost, speed, acc))

    if current_score &gt; best_dev:
        if data.seg:
            print(&quot;Exceed previous best f score:&quot;, best_dev)
        else:
            print(&quot;Exceed previous best acc score:&quot;, best_dev)
        model_name = data.model_dir +&#39;.&#39;+ str(idx) + &quot;.model&quot;
        print(&quot;Save current best model in file:&quot;, model_name)
        torch.save(model.state_dict(), model_name)
        best_dev = current_score
    # ## decode test
    speed, acc, p, r, f, _,_ = evaluate(data, model, &quot;test&quot;)
    test_finish = time.time()
    test_cost = test_finish - dev_finish
    if data.seg:
        print(&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot;%(test_cost, speed, acc, p, r, f))
    else:
        print(&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f&quot;%(test_cost, speed, acc))
    gc.collect()
</code></pre>
<p>def load_model_decode(data, name):<br>    print(“Load Model from file: “, data.model_dir)<br>    if data.sentence_classification:<br>        model = SentClassifier(data)<br>    else:<br>        model = SeqLabel(data)<br>    # model = SeqModel(data)<br>    ## load model need consider if the model trained in GPU and load in CPU, or vice versa<br>    # if not gpu:<br>    #     model.load_state_dict(torch.load(model_dir))<br>    #     # model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)<br>    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)<br>    # else:<br>    #     model.load_state_dict(torch.load(model_dir))<br>    #     # model = torch.load(model_dir)<br>    model.load_state_dict(torch.load(data.load_model_dir))</p>
<pre><code>print(&quot;Decode %s data, nbest: %s ...&quot;%(name, data.nbest))
start_time = time.time()
speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, name, data.nbest)
end_time = time.time()
time_cost = end_time - start_time
if data.seg:
    print(&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot;%(name, time_cost, speed, acc, p, r, f))
else:
    print(&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f&quot;%(name, time_cost, speed, acc))
return pred_results, pred_scores
</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    parser = argparse.ArgumentParser(description=’Tuning with NCRF++’)<br>    # parser.add_argument(‘–status’, choices=[‘train’, ‘decode’], help=’update algorithm’, default=’train’)<br>    parser.add_argument(‘–config’,  help=’Configuration File’, default=’None’)<br>    parser.add_argument(‘–wordemb’,  help=’Embedding for words’, default=’None’)<br>    parser.add_argument(‘–charemb’,  help=’Embedding for chars’, default=’None’)<br>    parser.add_argument(‘–status’, choices=[‘train’, ‘decode’], help=’update algorithm’, default=’train’)<br>    parser.add_argument(‘–savemodel’, default=”data/model/saved_model.lstmcrf.”)<br>    parser.add_argument(‘–savedset’, help=’Dir of saved data setting’)<br>    parser.add_argument(‘–train’, default=”data/conll03/train.bmes”)<br>    parser.add_argument(‘–dev’, default=”data/conll03/dev.bmes” )<br>    parser.add_argument(‘–test’, default=”data/conll03/test.bmes”)<br>    parser.add_argument(‘–seg’, default=”True”)<br>    parser.add_argument(‘–raw’)<br>    parser.add_argument(‘–loadmodel’)<br>    parser.add_argument(‘–output’) </p>
<pre><code>args = parser.parse_args()
data = Data()
data.HP_gpu = torch.cuda.is_available()
if args.config == &#39;None&#39;:
    data.train_dir = args.train 
    data.dev_dir = args.dev 
    data.test_dir = args.test
    data.model_dir = args.savemodel
    data.dset_dir = args.savedset
    print(&quot;Save dset directory:&quot;,data.dset_dir)
    save_model_dir = args.savemodel
    data.word_emb_dir = args.wordemb
    data.char_emb_dir = args.charemb
    if args.seg.lower() == &#39;true&#39;:
        data.seg = True
    else:
        data.seg = False
    print(&quot;Seed num:&quot;,seed_num)
else:
    data.read_config(args.config)
# data.show_data_summary()
status = data.status.lower()
print(&quot;Seed num:&quot;,seed_num)

if status == &#39;train&#39;:
    print(&quot;MODEL: train&quot;)
    data_initialization(data)
    data.generate_instance(&#39;train&#39;)
    data.generate_instance(&#39;dev&#39;)
    data.generate_instance(&#39;test&#39;)
    data.build_pretrain_emb()
    train(data)
elif status == &#39;decode&#39;:
    print(&quot;MODEL: decode&quot;)
    data.load(data.dset_dir)
    data.read_config(args.config)
    print(data.raw_dir)
    # exit(0)
    data.show_data_summary()
    data.generate_instance(&#39;raw&#39;)
    print(&quot;nbest: %s&quot;%(data.nbest))
    decode_results, pred_scores = load_model_decode(data, &#39;raw&#39;)
    if data.nbest and not data.sentence_classification:
        data.write_nbest_decoded_results(decode_results, pred_scores, &#39;raw&#39;)
    else:
        data.write_decoded_results(decode_results, &#39;raw&#39;)
else:
    print(&quot;Invalid argument! Please use valid arguments! (train/test/decode)&quot;)
</code></pre>
<p>‘’’</p>
]]></content>
      <categories>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>GNN</title>
    <url>/2022/02/21/GNN/</url>
    <content><![CDATA[<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><span id="more"></span>
<p>import time<br>import sys<br>import argparse<br>import random<br>import torch<br>import gc<br>import pickle<br>import os<br>import torch.autograd as autograd<br>import torch.optim as optim<br>import numpy as np<br>from utils.metric import get_ner_fmeasure<br>from model.LGN import Graph<br>from utils.data import Data</p>
<p>def str2bool(v):<br>    if isinstance(v, bool):<br>        return v<br>    if v.lower() in (‘yes’, ‘true’, ‘t’, ‘y’, ‘1’):<br>        return True<br>    elif v.lower() in (‘no’, ‘false’, ‘f’, ‘n’, ‘0’):<br>        return False<br>    else:<br>        raise argparse.ArgumentTypeError(‘Boolean value expected.’)</p>
<p>def lr_decay(optimizer, epoch, decay_rate, init_lr):<br>    lr = init_lr * ((1 - decay_rate) ** epoch)<br>    print(“ Learning rate is setted as:”, lr)<br>    for param_group in optimizer.param_groups:<br>        if param_group[‘name’] == ‘aggr’:<br>            param_group[‘lr’] = lr * 2.<br>        else:<br>            param_group[‘lr’] = lr<br>    return optimizer</p>
<p>def data_initialization(data, word_file, train_file, dev_file, test_file):<br>    data.build_word_file(word_file)</p>
<pre><code>if train_file:
    data.build_alphabet(train_file)
    data.build_word_alphabet(train_file)
if dev_file:
    data.build_alphabet(dev_file)
    data.build_word_alphabet(dev_file)
if test_file:
    data.build_alphabet(test_file)
    data.build_word_alphabet(test_file)
return data
</code></pre>
<p>def predict_check(pred_variable, gold_variable, mask_variable):<br>    pred = pred_variable.cpu().data.numpy()<br>    gold = gold_variable.cpu().data.numpy()<br>    mask = mask_variable.cpu().data.numpy()<br>    overlaped = (pred == gold)<br>    right_token = np.sum(overlaped * mask)<br>    total_token = mask.sum()<br>    return right_token, total_token</p>
<p>def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet):<br>    batch_size = gold_variable.size(0)<br>    seq_len = gold_variable.size(1)<br>    mask = mask_variable.cpu().data.numpy()<br>    pred_tag = pred_variable.cpu().data.numpy()<br>    gold_tag = gold_variable.cpu().data.numpy()<br>    pred_label = []<br>    gold_label = []</p>
<pre><code>for idx in range(batch_size):
    pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]
    gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]
    assert (len(pred) == len(gold))
    pred_label.append(pred)
    gold_label.append(gold)

return pred_label, gold_label
</code></pre>
<p>def print_args(args):<br>    print(“CONFIG SUMMARY:”)<br>    print(“     Batch size: %s” % (args.batch_size))<br>    print(“     If use GPU: %s” % (args.use_gpu))<br>    print(“     If use CRF: %s” % (args.use_crf))<br>    print(“     Epoch  number: %s” % (args.num_epoch))<br>    print(“     Learning rate: %s” % (args.lr))<br>    print(“     L2 normalization rate: %s” % (args.weight_decay))<br>    print(“     If use edge embedding: %s” % (args.use_edge))<br>    print(“     If  use  global  node: %s” % (args.use_global))<br>    print(“     Bidirectional digraph: %s” % (args.bidirectional))<br>    print(“     Update   step  number: %s” % (args.iters))<br>    print(“     Attention  dropout   rate: %s” % (args.tf_drop_rate))<br>    print(“     Embedding  dropout   rate: %s” % (args.emb_drop_rate))<br>    print(“     Hidden  state   dimension: %s” % (args.hidden_dim))<br>    print(“     Learning rate decay ratio: %s” % (args.lr_decay))<br>    print(“     Aggregation module dropout rate: %s” % (args.cell_drop_rate))<br>    print(“     Head    number   of   attention: %s” % (args.num_head))<br>    print(“     Head  dimension   of  attention: %s” % (args.head_dim))<br>    print(“CONFIG SUMMARY END.”)<br>    sys.stdout.flush()</p>
<p>def evaluate(data, args, model, name):<br>    if name == “train”:<br>        instances = data.train_Ids<br>    elif name == “dev”:<br>        instances = data.dev_Ids<br>    elif name == ‘test’:<br>        instances = data.test_Ids<br>    elif name == ‘raw’:<br>        instances = data.raw_Ids<br>    else:<br>        print(“Error: wrong evaluate name,”, name)<br>        exit(0)</p>
<pre><code>pred_results = []
gold_results = []

# set model in eval model
model.eval()
batch_size = args.batch_size
start_time = time.time()
train_num = len(instances)
total_batch = train_num // batch_size + 1

for batch_id in range(total_batch):
    start = batch_id * batch_size
    end = (batch_id + 1) * batch_size
    if end &gt; train_num:
        end = train_num
    instance = instances[start:end]
    if not instance:
        continue

    word_list, batch_char, batch_label, mask = batchify_with_label(instance, args.use_gpu)
    _, tag_seq = model(word_list, batch_char, mask)

    pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet)

    pred_results += pred_label
    gold_results += gold_label

decode_time = time.time() - start_time
speed = len(instances) / decode_time

acc, p, r, f = get_ner_fmeasure(gold_results, pred_results)
return speed, acc, p, r, f, pred_results
</code></pre>
<p>def batchify_with_label(input_batch_list, gpu):<br>    batch_size = len(input_batch_list)<br>    chars = [sent[0] for sent in input_batch_list]<br>    words = [sent[1] for sent in input_batch_list]<br>    labels = [sent[2] for sent in input_batch_list]</p>
<pre><code>sent_lengths = torch.LongTensor(list(map(len, chars)))
max_sent_len = sent_lengths.max()
char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_sent_len))).long()
label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_sent_len))).long()
mask = autograd.Variable(torch.zeros((batch_size, max_sent_len))).byte()

for idx, (seq, label, seq_len) in enumerate(zip(chars, labels, sent_lengths)):
    char_seq_tensor[idx, :seq_len] = torch.LongTensor(seq)
    label_seq_tensor[idx, :seq_len] = torch.LongTensor(label)
    mask[idx, :seq_len] = torch.Tensor([1] * int(seq_len))

if gpu:
    char_seq_tensor = char_seq_tensor.cuda()
    label_seq_tensor = label_seq_tensor.cuda()
    mask = mask.cuda()

return words, char_seq_tensor, label_seq_tensor, mask
</code></pre>
<p>def train(data, args, saved_model_path):<br>    print(“Training model…”)<br>    model = Graph(data, args)<br>    if args.use_gpu:<br>        model = model.cuda()<br>    print(‘# generated parameters:’, sum(param.numel() for param in model.parameters()))<br>    print(“Finished built model.”)</p>
<pre><code>best_dev_epoch = 0
best_dev_f = -1
best_dev_p = -1
best_dev_r = -1

best_test_f = -1
best_test_p = -1
best_test_r = -1

# Initialize the optimizer
aggr_module_params = []
other_module_params = []
for m_name in model._modules:
    m = model._modules[m_name]
    if isinstance(m, torch.nn.ModuleList):
        for p in m.parameters():
            if p.requires_grad:
                aggr_module_params.append(p)
    else:
        for p in m.parameters():
            if p.requires_grad:
                other_module_params.append(p)

optimizer = optim.Adam([
    &#123;&quot;params&quot;: (aggr_module_params), &quot;name&quot;: &quot;aggr&quot;&#125;,
    &#123;&quot;params&quot;: (other_module_params), &quot;name&quot;: &quot;other&quot;&#125;
],
    lr=args.lr,
    weight_decay=args.weight_decay
)

for idx in range(args.num_epoch):
    epoch_start = time.time()
    temp_start = epoch_start
    print((&quot;Epoch: %s/%s&quot; % (idx, args.num_epoch)))
    optimizer = lr_decay(optimizer, idx, args.lr_decay, args.lr)
    sample_loss = 0
    batch_loss = 0
    total_loss = 0
    right_token = 0
    whole_token = 0
    random.shuffle(data.train_Ids)
    # set model in train model
    model.train()
    model.zero_grad()
    batch_size = args.batch_size
    train_num = len(data.train_Ids)
    total_batch = train_num // batch_size + 1

    for batch_id in range(total_batch):
        # Get one batch-sized instance
        start = batch_id * batch_size
        end = (batch_id + 1) * batch_size
        if end &gt; train_num:
            end = train_num
        instance = data.train_Ids[start:end]
        if not instance:
            continue

        word_list, batch_char, batch_label, mask = batchify_with_label(instance, args.use_gpu)
        loss, tag_seq = model(word_list, batch_char, mask, batch_label)
        right, whole = predict_check(tag_seq, batch_label, mask)
        right_token += right
        whole_token += whole
        sample_loss += loss.data
        total_loss += loss.data
        batch_loss += loss

        if end % 500 == 0:
            temp_time = time.time()
            temp_cost = temp_time - temp_start
            temp_start = temp_time
            print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; %
                   (end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
            sys.stdout.flush()
            sample_loss = 0
        if end % args.batch_size == 0:
            batch_loss.backward()
            optimizer.step()
            model.zero_grad()
            batch_loss = 0

    temp_time = time.time()
    temp_cost = temp_time - temp_start
    print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; %
           (end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
    epoch_finish = time.time()
    epoch_cost = epoch_finish - epoch_start
    print((&quot;Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s&quot; %
           (idx, epoch_cost, train_num / epoch_cost, total_loss)))

    # dev
    speed, acc, dev_p, dev_r, dev_f, _ = evaluate(data, args, model, &quot;dev&quot;)
    dev_finish = time.time()
    dev_cost = dev_finish - epoch_finish

    print((&quot;Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; %
           (dev_cost, speed, acc, dev_p, dev_r, dev_f)))

    # test
    speed, acc, test_p, test_r, test_f, _ = evaluate(data, args, model, &quot;test&quot;)
    test_finish = time.time()
    test_cost = test_finish - dev_finish

    print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; %
           (test_cost, speed, acc, test_p, test_r, test_f)))

    if dev_f &gt; best_dev_f:
        print(&quot;Exceed previous best f score: %.4f&quot; % best_dev_f)
        torch.save(model.state_dict(), saved_model_path + &quot;_best&quot;)
        best_dev_p = dev_p
        best_dev_r = dev_r
        best_dev_f = dev_f
        best_dev_epoch = idx + 1
        best_test_p = test_p
        best_test_r = test_r
        best_test_f = test_f

    model_idx_path = saved_model_path + &quot;_&quot; + str(idx)
    torch.save(model.state_dict(), model_idx_path)
    with open(saved_model_path + &quot;_result.txt&quot;, &quot;a&quot;) as file:
        file.write(model_idx_path + &#39;\n&#39;)
        file.write(&quot;Dev score: %.4f, r: %.4f, f: %.4f\n&quot; % (dev_p, dev_r, dev_f))
        file.write(&quot;Test score: %.4f, r: %.4f, f: %.4f\n\n&quot; % (test_p, test_r, test_f))
        file.close()

    print(&quot;Best dev epoch: %d&quot; % best_dev_epoch)
    print(&quot;Best dev score: p: %.4f, r: %.4f, f: %.4f&quot; % (best_dev_p, best_dev_r, best_dev_f))
    print(&quot;Best test score: p: %.4f, r: %.4f, f: %.4f&quot; % (best_test_p, best_test_r, best_test_f))

    gc.collect()

with open(saved_model_path + &quot;_result.txt&quot;, &quot;a&quot;) as file:
    file.write(&quot;Best epoch: %d&quot; % best_dev_epoch + &#39;\n&#39;)
    file.write(&quot;Best Dev score: %.4f, r: %.4f, f: %.4f\n&quot; % (best_dev_p, best_dev_r, best_dev_f))
    file.write(&quot;Test score: %.4f, r: %.4f, f: %.4f\n\n&quot; % (best_test_p, best_test_r, best_test_f))
    file.close()

with open(saved_model_path + &quot;_best_HP.config&quot;, &quot;wb&quot;) as file:
    pickle.dump(args, file)
</code></pre>
<p>def load_model_decode(model_dir, data, args, name):<br>    model_dir = model_dir + “_best”<br>    print(“Load Model from file: “, model_dir)<br>    model = Graph(data, args)<br>    model.load_state_dict(torch.load(model_dir))</p>
<pre><code># load model need consider if the model trained in GPU and load in CPU, or vice versa
if args.use_gpu:
    model = model.cuda()

print((&quot;Decode %s data ...&quot; % name))
start_time = time.time()
speed, acc, p, r, f, pred_results = evaluate(data, args, model, name)
end_time = time.time()
time_cost = end_time - start_time
print((&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; %
       (name, time_cost, speed, acc, p, r, f)))

return pred_results
</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(‘–status’, choices=[‘train’, ‘test’, ‘decode’], help=’Function status.’, default=’train’)<br>    parser.add_argument(‘–use_gpu’, type=str2bool, default=False)<br>    parser.add_argument(‘–train’, help=’Training set.’, default=’data/onto4ner.cn/train.char.bmes’)<br>    parser.add_argument(‘–dev’, help=’Developing set.’, default=’data/onto4ner.cn/dev.char.bmes’)<br>    parser.add_argument(‘–test’, help=’Testing set.’, default=’data/onto4ner.cn/test.char.bmes’)<br>    parser.add_argument(‘–raw’, help=’Raw file for decoding.’)<br>    parser.add_argument(‘–output’, help=’Output results for decoding.’)<br>    parser.add_argument(‘–saved_set’, help=’Path of saved data set.’, default=’data/onto4ner.cn/saved.dset’)<br>    parser.add_argument(‘–saved_model’, help=’Path of saved model.’, default=”saved_model/model_onto4ner”)<br>    parser.add_argument(‘–char_emb’, help=’Path of character embedding file.’,<br>                        default=”data/gigaword_chn.all.a2b.uni.ite50.vec”)<br>    parser.add_argument(‘–word_emb’, help=’Path of word embedding file.’, default=”data/ctb.50d.vec”)</p>
<pre><code>parser.add_argument(&#39;--use_crf&#39;, type=str2bool, default=True)
parser.add_argument(&#39;--use_edge&#39;, type=str2bool, default=True, help=&#39;If use lexicon embeddings (edge embeddings).&#39;)
parser.add_argument(&#39;--use_global&#39;, type=str2bool, default=True, help=&#39;If use the global node.&#39;)
parser.add_argument(&#39;--bidirectional&#39;, type=str2bool, default=True, help=&#39;If use bidirectional digraph.&#39;)

parser.add_argument(&#39;--seed&#39;, help=&#39;Random seed&#39;, default=1023, type=int)
parser.add_argument(&#39;--batch_size&#39;, help=&#39;Batch size.&#39;, default=1, type=int)
parser.add_argument(&#39;--num_epoch&#39;, default=100, type=int, help=&quot;Epoch number.&quot;)
parser.add_argument(&#39;--iters&#39;, default=4, type=int, help=&#39;The number of Graph iterations.&#39;)
parser.add_argument(&#39;--hidden_dim&#39;, default=50, type=int, help=&#39;Hidden state size.&#39;)
parser.add_argument(&#39;--num_head&#39;, default=10, type=int, help=&#39;Number of transformer head.&#39;)
parser.add_argument(&#39;--head_dim&#39;, default=20, type=int, help=&#39;Head dimension of transformer.&#39;)
parser.add_argument(&#39;--tf_drop_rate&#39;, default=0.1, type=float, help=&#39;Transformer dropout rate.&#39;)
parser.add_argument(&#39;--emb_drop_rate&#39;, default=0.5, type=float, help=&#39;Embedding dropout rate.&#39;)
parser.add_argument(&#39;--cell_drop_rate&#39;, default=0.2, type=float, help=&#39;Aggregation module dropout rate.&#39;)
parser.add_argument(&#39;--word_alphabet_size&#39;, type=int, help=&#39;Word alphabet size.&#39;)
parser.add_argument(&#39;--char_alphabet_size&#39;, type=int, help=&#39;Char alphabet size.&#39;)
parser.add_argument(&#39;--label_alphabet_size&#39;, type=int, help=&#39;Label alphabet size.&#39;)
parser.add_argument(&#39;--char_dim&#39;, type=int, help=&#39;Char embedding size.&#39;)
parser.add_argument(&#39;--word_dim&#39;, type=int, help=&#39;Word embedding size.&#39;)
parser.add_argument(&#39;--lr&#39;, type=float, default=2e-05)
parser.add_argument(&#39;--lr_decay&#39;, type=float, default=0)
parser.add_argument(&#39;--weight_decay&#39;, type=float, default=0)

args = parser.parse_args()

status = args.status.lower()
seed_num = args.seed
random.seed(seed_num)
torch.manual_seed(seed_num)
np.random.seed(seed_num)

train_file = args.train
dev_file = args.dev
test_file = args.test
raw_file = args.raw
output_file = args.output
saved_set_path = args.saved_set
saved_model_path = args.saved_model
char_file = args.char_emb
word_file = args.word_emb

if status == &#39;train&#39;:
    if os.path.exists(saved_set_path):
        print(&#39;Loading saved data set...&#39;)
        with open(saved_set_path, &#39;rb&#39;) as f:
            data = pickle.load(f)
    else:
        data = Data()
        data_initialization(data, word_file, train_file, dev_file, test_file)
        data.generate_instance_with_words(train_file, &#39;train&#39;)
        data.generate_instance_with_words(dev_file, &#39;dev&#39;)
        data.generate_instance_with_words(test_file, &#39;test&#39;)
        data.build_char_pretrain_emb(char_file)
        data.build_word_pretrain_emb(word_file)
        if saved_set_path is not None:
            print(&#39;Dumping data...&#39;)
            with open(saved_set_path, &#39;wb&#39;) as f:
                pickle.dump(data, f)
    data.show_data_summary()
    args.word_alphabet_size = data.word_alphabet.size()
    args.char_alphabet_size = data.char_alphabet.size()
    args.label_alphabet_size = data.label_alphabet.size()
    args.char_dim = data.char_emb_dim
    args.word_dim = data.word_emb_dim
    print_args(args)
    train(data, args, saved_model_path)

elif status == &#39;test&#39;:
    assert not (test_file is None)
    if os.path.exists(saved_set_path):
        print(&#39;Loading saved data set...&#39;)
        with open(saved_set_path, &#39;rb&#39;) as f:
            data = pickle.load(f)
    else:
        print(&quot;Cannot find saved data set: &quot;, saved_set_path)
        exit(0)
    data.generate_instance_with_words(test_file, &#39;test&#39;)
    with open(saved_model_path + &quot;_best_HP.config&quot;, &quot;rb&quot;) as f:
        args = pickle.load(f)
    data.show_data_summary()
    print_args(args)
    load_model_decode(saved_model_path, data, args, &quot;test&quot;)

elif status == &#39;decode&#39;:
    assert not (raw_file is None or output_file is None)
    if os.path.exists(saved_set_path):
        print(&#39;Loading saved data set...&#39;)
        with open(saved_set_path, &#39;rb&#39;) as f:
            data = pickle.load(f)
    else:
        print(&quot;Cannot find saved data set: &quot;, saved_set_path)
        exit(0)
    data.generate_instance_with_words(raw_file, &#39;raw&#39;)
    with open(saved_model_path + &quot;_best_HP.config&quot;, &quot;rb&quot;) as f:
        args = pickle.load(f)
    data.show_data_summary()
    print_args(args)
    decode_results = load_model_decode(saved_model_path, data, args, &quot;raw&quot;)
    data.write_decoded_results(output_file, decode_results, &#39;raw&#39;)
else:
    print(&quot;Invalid argument! Please use valid arguments! (train/test/decode)&quot;)
</code></pre>
]]></content>
      <categories>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>LR-CNN</title>
    <url>/2022/02/21/LR-CNN/</url>
    <content><![CDATA[<p><img src="/2022/02/21/LR-CNN/lrcnn.png" alt="lrcnn"></p>
<span id="more"></span>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><p>‘’’</p>
<h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1><p>import time<br>import sys<br>import argparse<br>import random<br>import copy<br>import torch<br>import gc<br>import pickle<br>import os<br>import torch.autograd as autograd<br>import torch.nn as nn<br>import torch.nn.functional as F<br>import torch.optim as optim<br>import numpy as np<br>from utils.metric import get_ner_fmeasure<br>from model.CNNmodel import CNNmodel as SeqModel<br>from utils.data import Data</p>
<p>def data_initialization(data, gaz_file, train_file, dev_file, test_file):<br>    data.build_alphabet(train_file)<br>    data.build_alphabet(dev_file)<br>    data.build_alphabet(test_file)<br>    data.build_gaz_file(gaz_file)<br>    data.build_gaz_alphabet(train_file)<br>    data.build_gaz_alphabet(dev_file)<br>    data.build_gaz_alphabet(test_file)<br>    data.fix_alphabet()<br>    return data</p>
<p>def predict_check(pred_variable, gold_variable, mask_variable):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result, in numpy format<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””</p>
<pre><code>pred = pred_variable.cpu().data.numpy()
gold = gold_variable.cpu().data.numpy()
mask = mask_variable.cpu().data.numpy()
overlaped = (pred == gold)
right_token = np.sum(overlaped * mask)
total_token = mask.sum()
return right_token, total_token
</code></pre>
<p>def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””<br>    batch_size = gold_variable.size(0)<br>    seq_len = gold_variable.size(1)<br>    mask = mask_variable.cpu().data.numpy()<br>    pred_tag = pred_variable.cpu().data.numpy()<br>    gold_tag = gold_variable.cpu().data.numpy()<br>    batch_size = mask.shape[0]<br>    pred_label = []<br>    gold_label = []<br>    for idx in range(batch_size):<br>        pred = [label_alphabet.get_instance(int(pred_tag[idx][idy])) for idy in range(seq_len) if mask[idx][idy] != 0]<br>        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]<br>        assert (len(pred) == len(gold))<br>        pred_label.append(pred)<br>        gold_label.append(gold)</p>
<pre><code>return pred_label, gold_label
</code></pre>
<p>def save_data_setting(data, save_file):<br>    new_data = copy.deepcopy(data)<br>    ## remove input instances<br>    new_data.train_texts = []<br>    new_data.dev_texts = []<br>    new_data.test_texts = []<br>    new_data.raw_texts = []</p>
<pre><code>new_data.train_Ids = []
new_data.dev_Ids = []
new_data.test_Ids = []
new_data.raw_Ids = []
## save data settings
with open(save_file, &#39;wb&#39;) as fp:
    pickle.dump(new_data, fp)
print(&quot;Data setting saved to file: &quot;, save_file)
</code></pre>
<p>def load_data_setting(save_file):<br>    with open(save_file, ‘rb’) as fp:<br>        data = pickle.load(fp)<br>    print(“Data setting loaded from file: “, save_file)<br>    data.show_data_summary()<br>    return data</p>
<p>def lr_decay(optimizer, epoch, decay_rate, init_lr):<br>    lr = init_lr * ((1 - decay_rate) ** epoch)<br>    print(“ Learning rate is setted as:”, lr)<br>    for param_group in optimizer.param_groups:<br>        param_group[‘lr’] = lr<br>    return optimizer</p>
<p>def set_seed(seed_num=1023):<br>    random.seed(seed_num)<br>    torch.manual_seed(seed_num)<br>    np.random.seed(seed_num)</p>
<p>def evaluate(data, model, name):<br>    if name == “train”:<br>        instances = data.train_Ids<br>    elif name == “dev”:<br>        instances = data.dev_Ids<br>    elif name == ‘test’:<br>        instances = data.test_Ids<br>    elif name == ‘raw’:<br>        instances = data.raw_Ids<br>    else:<br>        print(“Error: wrong evaluate name,”, name)<br>    right_token = 0<br>    whole_token = 0<br>    pred_results = []<br>    gold_results = []<br>    ## set model in eval model<br>    model.eval()<br>    batch_size = 1<br>    start_time = time.time()<br>    train_num = len(instances)<br>    total_batch = train_num // batch_size + 1<br>    gazes = []<br>    for batch_id in range(total_batch):<br>        start = batch_id * batch_size<br>        end = (batch_id + 1) * batch_size<br>        if end &gt; train_num:<br>            end = train_num<br>        instance = instances[start:end]<br>        if not instance:<br>            continue<br>        gaz_list, batch_word, batch_biword, batch_wordlen, batch_label, layer_gaz, gaz_mask, mask = batchify_with_label(<br>            instance, data.HP_gpu, data.HP_num_layer, True)</p>
<pre><code>    tag_seq = model(gaz_list, batch_word, batch_biword, batch_wordlen, layer_gaz, gaz_mask, mask)

    if name == &quot;dev&quot;:
        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet)
    else:
        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet)
    pred_results += pred_label
    gold_results += gold_label
decode_time = time.time() - start_time
speed = len(instances) / decode_time
acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)

return speed, acc, p, r, f, pred_results
</code></pre>
<p>def batchify_with_label(input_batch_list, gpu, num_layer, volatile_flag=False):<br>    batch_size = len(input_batch_list)<br>    words = [sent[0] for sent in input_batch_list]<br>    biwords = [sent[1] for sent in input_batch_list]<br>    gazs = [sent[3] for sent in input_batch_list]<br>    labels = [sent[4] for sent in input_batch_list]<br>    layer_gazs = [sent[5] for sent in input_batch_list]<br>    gaz_mask = [sent[6] for sent in input_batch_list]</p>
<pre><code>word_seq_lengths = torch.LongTensor(list(map(len, words)))
max_seq_len = word_seq_lengths.max()
word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()
biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()
label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()
layer_gaz_tensor = torch.zeros(batch_size, max_seq_len, num_layer).long()
mask = autograd.Variable(torch.zeros((batch_size, max_seq_len))).byte()
gaz_mask_tensor = torch.zeros((batch_size, max_seq_len, num_layer)).byte()

for idx, (seq, biseq, label, seqlen, layergaz, gazmask) in enumerate(
        zip(words, biwords, labels, word_seq_lengths, layer_gazs, gaz_mask)):
    word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)
    biword_seq_tensor[idx, :seqlen] = torch.LongTensor(biseq)
    label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)
    layer_gaz_tensor[idx, :seqlen] = torch.LongTensor(layergaz)
    mask[idx, :seqlen] = torch.Tensor([1] * int(seqlen))
    gaz_mask_tensor[idx, :seqlen] = torch.LongTensor(gazmask)


if gpu:
    word_seq_tensor = word_seq_tensor.cuda()
    biword_seq_tensor = biword_seq_tensor.cuda()
    word_seq_lengths = word_seq_lengths.cuda()
    label_seq_tensor = label_seq_tensor.cuda()
    layer_gaz_tensor = layer_gaz_tensor.cuda()
    gaz_mask_tensor = gaz_mask_tensor.cuda()
    mask = mask.cuda()
return gazs, word_seq_tensor, biword_seq_tensor, word_seq_lengths, label_seq_tensor, layer_gaz_tensor, gaz_mask_tensor, mask
</code></pre>
<p>def train(data, save_model_dir, seg=True):<br>    print(“Training model…”)</p>
<pre><code>model = SeqModel(data)
print(&quot;finish building model.&quot;)
parameters = filter(lambda p: p.requires_grad, model.parameters())
optimizer = optim.Adamax(parameters, lr=data.HP_lr)
best_dev = -1
best_dev_p = -1
best_dev_r = -1

best_test = -1
best_test_p = -1
best_test_r = -1

## start training
for idx in range(data.HP_iteration):
    epoch_start = time.time()
    temp_start = epoch_start
    print((&quot;Epoch: %s/%s&quot; % (idx, data.HP_iteration)))
    optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)
    instance_count = 0
    sample_id = 0
    sample_loss = 0
    batch_loss = 0
    total_loss = 0
    right_token = 0
    whole_token = 0
    random.shuffle(data.train_Ids)
    ## set model in train model
    model.train()
    model.zero_grad()
    batch_size = data.HP_batch_size
    batch_id = 0
    train_num = len(data.train_Ids)
    total_batch = train_num // batch_size + 1
    for batch_id in range(total_batch):
        start = batch_id * batch_size
        end = (batch_id + 1) * batch_size
        if end &gt; train_num:
            end = train_num
        instance = data.train_Ids[start:end]
        if not instance:
            continue
        gaz_list, batch_word, batch_biword, batch_wordlen, batch_label, layer_gaz, gaz_mask, mask = batchify_with_label(
            instance, data.HP_gpu, data.HP_num_layer)
        gaz_mask, mask = gaz_mask.bool(), mask.bool()
        instance_count += 1
        loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, layer_gaz,
                                                      gaz_mask, mask, batch_label)

        right, whole = predict_check(tag_seq, batch_label, mask)
        right_token += right
        whole_token += whole
        sample_loss += loss.data
        total_loss += loss.data
        batch_loss += loss

        if end % 500 == 0:
            temp_time = time.time()
            temp_cost = temp_time - temp_start
            temp_start = temp_time
            print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; % (
            end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
            sys.stdout.flush()
            sample_loss = 0
        if end % data.HP_batch_size == 0:
            batch_loss.backward()
            optimizer.step()
            model.zero_grad()
            batch_loss = 0

    temp_time = time.time()
    temp_cost = temp_time - temp_start
    print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; % (
    end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
    epoch_finish = time.time()
    epoch_cost = epoch_finish - epoch_start
    print((&quot;Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s&quot; % (
    idx, epoch_cost, train_num / epoch_cost, total_loss)))

    speed, acc, p, r, f, pred_labels = evaluate(data, model, &quot;dev&quot;)
    dev_finish = time.time()
    dev_cost = dev_finish - epoch_finish

    if seg:
        current_score = f
        print((&quot;Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
        dev_cost, speed, acc, p, r, f)))
    else:
        current_score = acc
        print((&quot;Dev: time: %.2fs speed: %.2fst/s; acc: %.4f&quot; % (dev_cost, speed, acc)))

    if current_score &gt; best_dev:
        if seg:
            print(&quot;Exceed previous best f score:&quot;, best_dev)

        else:
            print(&quot;Exceed previous best acc score:&quot;, best_dev)

        model_name = save_model_dir
        torch.save(model.state_dict(), model_name)
        best_dev_p = p
        best_dev_r = r

    # ## decode test
    speed, acc, p, r, f, pred_labels = evaluate(data, model, &quot;test&quot;)
    test_finish = time.time()
    test_cost = test_finish - dev_finish
    if seg:
        current_test_score = f
        print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
        test_cost, speed, acc, p, r, f)))
    else:
        current_test_score = acc
        print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f&quot; % (test_cost, speed, acc)))

    if current_score &gt; best_dev:
        best_dev = current_score
        best_test = current_test_score
        best_test_p = p
        best_test_r = r

    print(&quot;Best dev score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;&quot;.format(best_dev_p, best_dev_r, best_dev))
    print(&quot;Test score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;&quot;.format(best_test_p, best_test_r, best_test))
    gc.collect()

with open(data.result_file, &quot;a&quot;) as f:
    f.write(save_model_dir + &#39;\n&#39;)
    f.write(&quot;Best dev score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;\n&quot;.format(best_dev_p, best_dev_r, best_dev))
    f.write(&quot;Test score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;\n\n&quot;.format(best_test_p, best_test_r, best_test))
    f.close()
</code></pre>
<p>def load_model_decode(model_dir, data, name, gpu, seg=True):<br>    data.HP_gpu = gpu<br>    print(“Load Model from file: “, model_dir)<br>    model = SeqModel(data)<br>    model.load_state_dict(torch.load(model_dir))</p>
<pre><code>print((&quot;Decode %s data ...&quot; % (name)))
start_time = time.time()
speed, acc, p, r, f, pred_results = evaluate(data, model, name)
end_time = time.time()
time_cost = end_time - start_time
if seg:
    print((&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
    name, time_cost, speed, acc, p, r, f)))
else:
    print((&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f&quot; % (name, time_cost, speed, acc)))

return pred_results
</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(‘–embedding’, help=’Embedding for words’, default=’None’)<br>    parser.add_argument(‘–status’, choices=[‘train’, ‘test’], help=’update algorithm’, default=’train’)<br>    parser.add_argument(‘–modelpath’, default=”save_model/“)<br>    parser.add_argument(‘–modelname’, default=”model”)<br>    parser.add_argument(‘–savedset’, help=’Dir of saved data setting’, default=”data/save.dset”)<br>    parser.add_argument(‘–train’, default=”ResumeNER/train.char.bmes”)<br>    parser.add_argument(‘–dev’, default=”ResumeNER/dev.char.bmes”)<br>    parser.add_argument(‘–test’, default=”ResumeNER/test.char.bmes”)<br>    parser.add_argument(‘–seg’, default=”True”)<br>    parser.add_argument(‘–extendalphabet’, default=”True”)<br>    parser.add_argument(‘–raw’)</p>
<pre><code>parser.add_argument(&#39;--output&#39;)
parser.add_argument(&#39;--seed&#39;, default=1023, type=int)
parser.add_argument(&#39;--labelcomment&#39;, default=&quot;&quot;)
parser.add_argument(&#39;--resultfile&#39;, default=&quot;result/result.txt&quot;)
parser.add_argument(&#39;--num_iter&#39;, default=10, type=int)
parser.add_argument(&#39;--num_layer&#39;, default=4, type=int)
parser.add_argument(&#39;--lr&#39;, type=float, default=0.0015)
parser.add_argument(&#39;--batch_size&#39;, type=int, default=1)
parser.add_argument(&#39;--hidden_dim&#39;, type=int, default=128)
parser.add_argument(&#39;--rethink_iter&#39;, type=int, default=2)

flag_parser = parser.add_mutually_exclusive_group(required=False)
flag_parser.add_argument(&#39;--use_biword&#39;, dest=&#39;use_biword&#39;, action=&#39;store_true&#39;)
flag_parser.add_argument(&#39;--no-use_biword&#39;, dest=&#39;use_biword&#39;, action=&#39;store_false&#39;)
parser.set_defaults(use_biword=False)

posi_parser = parser.add_mutually_exclusive_group(required=False)
posi_parser.add_argument(&#39;--use_posi&#39;, dest=&#39;use_posi&#39;, action=&#39;store_true&#39;)
posi_parser.add_argument(&#39;--no-use_posi&#39;, dest=&#39;use_posi&#39;, action=&#39;store_false&#39;)
parser.set_defaults(use_posi=True)

args = parser.parse_args()

seed_num = args.seed
set_seed(seed_num)

train_file = args.train
dev_file = args.dev
test_file = args.test
raw_file = args.raw
# model_dir = args.loadmodel
output_file = args.output
if args.seg.lower() == &quot;true&quot;:
    seg = True
else:
    seg = False
status = args.status.lower()

save_model_dir = args.modelpath + args.modelname
save_data_name = args.savedset
gpu = torch.cuda.is_available()

char_emb = &quot;./data/gigaword_chn.all.a2b.uni.ite50.vec&quot;
bichar_emb = None
gaz_file = &quot;./data/ctb.50d.vec&quot;
print(&quot;CuDNN:&quot;, torch.backends.cudnn.enabled)
# gpu = False
print(&quot;GPU available:&quot;, gpu)
print(&quot;Status:&quot;, status)
print(&quot;Seg: &quot;, seg)
print(&quot;Train file:&quot;, train_file)
print(&quot;Dev file:&quot;, dev_file)
print(&quot;Test file:&quot;, test_file)
print(&quot;Raw file:&quot;, raw_file)
print(&quot;Char emb:&quot;, char_emb)
print(&quot;Bichar emb:&quot;, bichar_emb)
print(&quot;Gaz file:&quot;, gaz_file)
sys.stdout.flush()

if status == &#39;train&#39;:
    if os.path.exists(save_data_name):
        print(&#39;Loading processed data&#39;)
        with open(save_data_name, &#39;rb&#39;) as fp:
            data = pickle.load(fp)
        data.HP_num_layer = args.num_layer
        data.HP_batch_size = args.batch_size
        data.HP_iteration = args.num_iter
        data.label_comment = args.labelcomment
        data.result_file = args.resultfile
        data.HP_lr = args.lr
        data.use_bigram = args.use_biword
        data.HP_hidden_dim = args.hidden_dim
        data.HP_use_posi = args.use_posi
        data.HP_rethink_iter = args.rethink_iter

    else:
        data = Data()
        data.HP_gpu = gpu
        data.HP_batch_size = args.batch_size
        data.HP_num_layer = args.num_layer
        data.HP_iteration = args.num_iter
        data.use_bigram = args.use_biword
        data.gaz_dropout = 0.5
        data.norm_gaz_emb = False
        data.HP_fix_gaz_emb = False
        data.label_comment = args.labelcomment
        data.result_file = args.resultfile
        data.HP_lr = args.lr
        data.HP_hidden_dim = args.hidden_dim
        data.HP_use_posi = args.use_posi
        data.HP_rethink_iter = args.rethink_iter
        data_initialization(data, gaz_file, train_file, dev_file, test_file)
        data.generate_instance_with_gaz(train_file, &#39;train&#39;)
        data.generate_instance_with_gaz(dev_file, &#39;dev&#39;)
        data.generate_instance_with_gaz(test_file, &#39;test&#39;)
        data.build_word_pretrain_emb(char_emb)
        data.build_biword_pretrain_emb(bichar_emb)
        data.build_gaz_pretrain_emb(gaz_file)

        print(&#39;Dumping data&#39;)
        with open(save_data_name, &#39;wb&#39;) as f:
            pickle.dump(data, f)
        set_seed(seed_num)
    data.show_data_summary()
    train(data, save_model_dir, seg)
elif status == &#39;test&#39;:
    print(&#39;Loading processed data&#39;)
    with open(save_data_name, &#39;rb&#39;) as fp:
        data = pickle.load(fp)
    data.HP_num_layer = args.num_layer
    data.HP_iteration = args.num_iter
    data.label_comment = args.labelcomment
    data.result_file = args.resultfile
    data.HP_lr = args.lr
    data.use_bigram = args.use_biword
    data.HP_use_posi = args.use_posi
    data.HP_rethink_iter = args.rethink_iter
    data.generate_instance_with_gaz(test_file, &#39;test&#39;)
    load_model_decode(save_model_dir, data, &#39;test&#39;, gpu, seg)
else:
    print(&quot;Invalid argument! Please use valid arguments! (train/test)&quot;)
</code></pre>
<p>‘’’</p>
]]></content>
      <categories>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>LatticeLSTM</title>
    <url>/2022/02/21/LatticeLSTM/</url>
    <content><![CDATA[<p><img src="/2022/02/21/LatticeLSTM/lattice_lstm.png" alt="lattice_lstm"></p>
<span id="more"></span>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><p>‘’’<br>import time<br>import sys<br>import argparse<br>import random<br>import copy<br>import torch<br>import gc<br>import pickle as pickle<br>import torch.autograd as autograd<br>import torch.nn as nn<br>import torch.nn.functional as F<br>import torch.optim as optim<br>import numpy as np<br>from utils.metric import get_ner_fmeasure<br>from model.bilstmcrf import BiLSTM_CRF as SeqModel<br>from utils.data import Data</p>
<p>seed_num = 100<br>random.seed(seed_num)<br>torch.manual_seed(seed_num)<br>np.random.seed(seed_num)</p>
<p>def data_initialization(data, gaz_file, train_file, dev_file, test_file):<br>    data.build_alphabet(train_file)<br>    data.build_alphabet(dev_file)<br>    data.build_alphabet(test_file)<br>    data.build_gaz_file(gaz_file)<br>    data.build_gaz_alphabet(train_file)<br>    data.build_gaz_alphabet(dev_file)<br>    data.build_gaz_alphabet(test_file)<br>    data.fix_alphabet()<br>    return data</p>
<p>def predict_check(pred_variable, gold_variable, mask_variable):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result, in numpy format<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””<br>    pred = pred_variable.cpu().data.numpy()<br>    gold = gold_variable.cpu().data.numpy()<br>    mask = mask_variable.cpu().data.numpy()<br>    overlaped = (pred == gold)<br>    right_token = np.sum(overlaped * mask)<br>    total_token = mask.sum()<br>    # print(“right: %s, total: %s”%(right_token, total_token))<br>    return right_token, total_token</p>
<p>def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””</p>
<pre><code>pred_variable = pred_variable[word_recover]
gold_variable = gold_variable[word_recover]
mask_variable = mask_variable[word_recover]
batch_size = gold_variable.size(0)
seq_len = gold_variable.size(1)
mask = mask_variable.cpu().data.numpy()
pred_tag = pred_variable.cpu().data.numpy()
gold_tag = gold_variable.cpu().data.numpy()
batch_size = mask.shape[0]
pred_label = []
gold_label = []
for idx in range(batch_size):
    pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]
    gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]
    # print &quot;p:&quot;,pred, pred_tag.tolist()
    # print &quot;g:&quot;, gold, gold_tag.tolist()
    assert (len(pred) == len(gold))
    pred_label.append(pred)
    gold_label.append(gold)
return pred_label, gold_label
</code></pre>
<p>def save_data_setting(data, save_file):<br>    new_data = copy.deepcopy(data)<br>    # remove input instances<br>    new_data.train_texts = []<br>    new_data.dev_texts = []<br>    new_data.test_texts = []<br>    new_data.raw_texts = []</p>
<pre><code>new_data.train_Ids = []
new_data.dev_Ids = []
new_data.test_Ids = []
new_data.raw_Ids = []
# save data settings
with open(save_file, &#39;wb&#39;) as fp:
    pickle.dump(new_data, fp)
print(&quot;Data setting saved to file: &quot;, save_file)
</code></pre>
<p>def load_data_setting(save_file):<br>    with open(save_file, ‘rb’) as fp:<br>        data = pickle.load(fp)<br>    print(“Data setting loaded from file: “, save_file)<br>    data.show_data_summary()<br>    return data</p>
<p>def lr_decay(optimizer, epoch, decay_rate, init_lr):<br>    lr = init_lr * ((1 - decay_rate) ** epoch)<br>    print(“ Learning rate is setted as:”, lr)<br>    for param_group in optimizer.param_groups:<br>        param_group[‘lr’] = lr<br>    return optimizer</p>
<p>def evaluate(data, model, name):<br>    if name == “train”:<br>        instances = data.train_Ids<br>    elif name == “dev”:<br>        instances = data.dev_Ids<br>    elif name == ‘test’:<br>        instances = data.test_Ids<br>    elif name == ‘raw’:<br>        instances = data.raw_Ids<br>    else:<br>        print(“Error: wrong evaluate name,”, name)<br>    right_token = 0<br>    whole_token = 0<br>    pred_results = []<br>    gold_results = []<br>    # set model in eval model<br>    model.eval()<br>    batch_size = 1<br>    start_time = time.time()<br>    train_num = len(instances)<br>    total_batch = train_num // batch_size + 1<br>    for batch_id in range(total_batch):<br>        start = batch_id * batch_size<br>        end = (batch_id + 1) * batch_size<br>        if end &gt; train_num:<br>            end = train_num<br>        instance = instances[start:end]<br>        if not instance:<br>            continue<br>        gaz_list, batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask = batchify_with_label(<br>            instance, data.HP_gpu, True)<br>        tag_seq = model(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover,<br>                        mask)<br>        # print “tag:”,tag_seq<br>        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)<br>        pred_results += pred_label<br>        gold_results += gold_label<br>    decode_time = time.time() - start_time<br>    speed = len(instances) / decode_time<br>    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)<br>    return speed, acc, p, r, f, pred_results</p>
<p>def batchify_with_label(input_batch_list, gpu, volatile_flag=False):<br>    “””<br>        input: list of words, chars and labels, various length. [[words,biwords,chars,gaz, labels],[words,biwords,chars,labels],…]<br>            words: word ids for one sentence. (batch_size, sent_len)<br>            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)<br>        output:<br>            zero padding for word and char, with their batch length<br>            word_seq_tensor: (batch_size, max_sent_len) Variable<br>            word_seq_lengths: (batch_size,1) Tensor<br>            char_seq_tensor: (batch_size<em>max_sent_len, max_word_len) Variable<br>            char_seq_lengths: (batch_size</em>max_sent_len,1) Tensor<br>            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order<br>            label_seq_tensor: (batch_size, max_sent_len)<br>            mask: (batch_size, max_sent_len)<br>    “””<br>    batch_size = len(input_batch_list)<br>    words = [sent[0] for sent in input_batch_list]<br>    biwords = [sent[1] for sent in input_batch_list]<br>    chars = [sent[2] for sent in input_batch_list]<br>    gazs = [sent[3] for sent in input_batch_list]<br>    labels = [sent[4] for sent in input_batch_list]<br>    word_seq_lengths = torch.LongTensor(list(map(len, words)))<br>    max_seq_len = word_seq_lengths.max()<br>    with torch.no_grad():<br>        word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()<br>        biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()<br>        label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()<br>        mask = autograd.Variable(torch.zeros((batch_size, max_seq_len))).bool()<br>    for idx, (seq, biseq, label, seqlen) in enumerate(zip(words, biwords, labels, word_seq_lengths)):<br>        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)<br>        biword_seq_tensor[idx, :seqlen] = torch.LongTensor(biseq)<br>        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)<br>        mask[idx, :seqlen] = torch.Tensor([1] * seqlen)<br>    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)<br>    word_seq_tensor = word_seq_tensor[word_perm_idx]<br>    biword_seq_tensor = biword_seq_tensor[word_perm_idx]<br>    ## not reorder label<br>    label_seq_tensor = label_seq_tensor[word_perm_idx]<br>    mask = mask[word_perm_idx]<br>    ### deal with char<br>    # pad_chars (batch_size, max_seq_len)<br>    pad_chars = [chars[idx] + [[0]] * (max_seq_len - len(chars[idx])) for idx in range(len(chars))]<br>    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]<br>    max_word_len = max(list(map(max, length_list)))<br>    with torch.no_grad():<br>        char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len))).long()<br>    char_seq_lengths = torch.LongTensor(length_list)<br>    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):<br>        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):<br>            # print len(word), wordlen<br>            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)<br>    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size * max_seq_len, -1)<br>    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size * max_seq_len, )<br>    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)<br>    char_seq_tensor = char_seq_tensor[char_perm_idx]<br>    _, char_seq_recover = char_perm_idx.sort(0, descending=False)<br>    _, word_seq_recover = word_perm_idx.sort(0, descending=False)</p>
<pre><code># keep the gaz_list in orignial order

gaz_list = [gazs[i] for i in word_perm_idx]
gaz_list.append(volatile_flag)
if gpu:
    word_seq_tensor = word_seq_tensor.cuda()
    biword_seq_tensor = biword_seq_tensor.cuda()
    word_seq_lengths = word_seq_lengths.cuda()
    word_seq_recover = word_seq_recover.cuda()
    label_seq_tensor = label_seq_tensor.cuda()
    char_seq_tensor = char_seq_tensor.cuda()
    char_seq_recover = char_seq_recover.cuda()
    mask = mask.cuda()
return gaz_list, word_seq_tensor, biword_seq_tensor, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask
</code></pre>
<p>def train(data, save_model_dir, seg=True):<br>    print(“Training model…”)<br>    data.show_data_summary()<br>    save_data_name = save_model_dir + “.dset”<br>    save_data_setting(data, save_data_name)<br>    model = SeqModel(data)<br>    print(“finished built model.”)<br>    loss_function = nn.NLLLoss()<br>    parameters = [p for p in model.parameters() if p.requires_grad]<br>    optimizer = optim.SGD(parameters, lr=data.HP_lr, momentum=data.HP_momentum)<br>    best_dev = -1<br>    data.HP_iteration = 10<br>    ## start training<br>    for idx in range(data.HP_iteration):<br>        epoch_start = time.time()<br>        temp_start = epoch_start<br>        print((“Epoch: %s/%s” % (idx, data.HP_iteration)))<br>        optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)<br>        instance_count = 0<br>        sample_id = 0<br>        sample_loss = 0<br>        batch_loss = 0<br>        total_loss = 0<br>        right_token = 0<br>        whole_token = 0<br>        random.shuffle(data.train_Ids)<br>        # set model in train model<br>        model.train()<br>        model.zero_grad()<br>        # current only support batch size = 1 to compulate and accumulate to data.HP_batch_size update weights<br>        batch_size = 1<br>        batch_id = 0<br>        train_num = len(data.train_Ids)<br>        total_batch = train_num // batch_size + 1<br>        for batch_id in range(total_batch):<br>            start = batch_id * batch_size<br>            end = (batch_id + 1) * batch_size<br>            if end &gt; train_num:<br>                end = train_num<br>            instance = data.train_Ids[start:end]<br>            if not instance:<br>                continue<br>            gaz_list, batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask = batchify_with_label(<br>                instance, data.HP_gpu)<br>            # print “gaz_list:”,gaz_list<br>            # exit(0)<br>            instance_count += 1<br>            loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char,<br>                                                          batch_charlen, batch_charrecover, batch_label, mask)<br>            right, whole = predict_check(tag_seq, batch_label, mask)<br>            right_token += right<br>            whole_token += whole<br>            sample_loss += loss.item()<br>            total_loss += loss.item()<br>            batch_loss += loss</p>
<pre><code>        if end % 500 == 0:
            temp_time = time.time()
            temp_cost = temp_time - temp_start
            temp_start = temp_time
            print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; % (
                end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
            sys.stdout.flush()
            sample_loss = 0
        if end % data.HP_batch_size == 0:
            batch_loss.backward()
            optimizer.step()
            model.zero_grad()
            batch_loss = 0
    temp_time = time.time()
    temp_cost = temp_time - temp_start
    print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; % (
        end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
    epoch_finish = time.time()
    epoch_cost = epoch_finish - epoch_start
    print((&quot;Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s&quot; % (
        idx, epoch_cost, train_num / epoch_cost, total_loss)))
    # exit(0)
    # continue
    speed, acc, p, r, f, _ = evaluate(data, model, &quot;dev&quot;)
    dev_finish = time.time()
    dev_cost = dev_finish - epoch_finish

    if seg:
        current_score = f
        print((&quot;Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
            dev_cost, speed, acc, p, r, f)))
    else:
        current_score = acc
        print((&quot;Dev: time: %.2fs speed: %.2fst/s; acc: %.4f&quot; % (dev_cost, speed, acc)))

    if current_score &gt; best_dev:
        if seg:
            print(&quot;Exceed previous best f score:&quot;, best_dev)
        else:
            print(&quot;Exceed previous best acc score:&quot;, best_dev)
        model_name = save_model_dir + &#39;.&#39; + str(idx) + &quot;.model&quot;
        torch.save(model.state_dict(), model_name)
        best_dev = current_score
        # ## decode test
    speed, acc, p, r, f, _ = evaluate(data, model, &quot;test&quot;)
    test_finish = time.time()
    test_cost = test_finish - dev_finish
    if seg:
        print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
            test_cost, speed, acc, p, r, f)))
    else:
        print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f&quot; % (test_cost, speed, acc)))
    gc.collect()
</code></pre>
<p>def load_model_decode(model_dir, data, name, gpu, seg=True):<br>    data.HP_gpu = gpu<br>    print(“Load Model from file: “, model_dir)<br>    model = SeqModel(data)<br>    # load model need consider if the model trained in GPU and load in CPU, or vice versa<br>    if not gpu:<br>        model.load_state_dict(torch.load(model_dir, map_location=lambda storage, loc: storage))<br>        # model = torch.load(model_dir, map_location=lambda storage, loc: storage)<br>    else:<br>        model.load_state_dict(torch.load(model_dir))<br>    # model = torch.load(model_dir)</p>
<pre><code>print((&quot;Decode %s data ...&quot; % name))
start_time = time.time()
speed, acc, p, r, f, pred_results = evaluate(data, model, name)
end_time = time.time()
time_cost = end_time - start_time
if seg:
    print((&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
        name, time_cost, speed, acc, p, r, f)))
else:
    print((&quot;%s: time:%.2fs, speed:%.2fst/s; acc: %.4f&quot; % (name, time_cost, speed, acc)))
return pred_results
</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    parser = argparse.ArgumentParser(description=’Tuning with bi-directional LSTM-CRF’)<br>    parser.add_argument(‘–embedding’, help=’Embedding for words’, default=’None’)<br>    parser.add_argument(‘–status’, choices=[‘train’, ‘test’, ‘decode’], help=’update algorithm’, default=’train’)<br>    parser.add_argument(‘–savemodel’, default=”data/model/saved_model.lstmcrf.”)<br>    parser.add_argument(‘–savedset’, help=’Dir of saved data setting’, default=”data/save.dset”)<br>    parser.add_argument(‘–train’, default=”data/conll03/train.bmes”)<br>    parser.add_argument(‘–dev’, default=”data/conll03/dev.bmes”)<br>    parser.add_argument(‘–test’, default=”data/conll03/test.bmes”)<br>    parser.add_argument(‘–seg’, default=”True”)<br>    parser.add_argument(‘–extendalphabet’, default=”True”)<br>    parser.add_argument(‘–raw’)<br>    parser.add_argument(‘–loadmodel’)<br>    parser.add_argument(‘–output’)<br>    args = parser.parse_args()</p>
<pre><code>train_file = args.train
dev_file = args.dev
test_file = args.test
raw_file = args.raw
model_dir = args.loadmodel
dset_dir = args.savedset
output_file = args.output
if args.seg.lower() == &quot;true&quot;:
    seg = True
else:
    seg = False
status = args.status.lower()

save_model_dir = args.savemodel
gpu = torch.cuda.is_available()

char_emb = &quot;LatticeLSTM/data/gigaword_chn.all.a2b.uni.ite50.vec&quot;
bichar_emb = None
gaz_file = &quot;LatticeLSTM/data/ctb.50d.vec&quot;
# gaz_file = None
# char_emb = None
# bichar_emb = None

print(&quot;CuDNN:&quot;, torch.backends.cudnn.enabled)
# gpu = False
print(&quot;GPU available:&quot;, gpu)
print(&quot;Status:&quot;, status)
print(&quot;Seg: &quot;, seg)
print(&quot;Train file:&quot;, train_file)
print(&quot;Dev file:&quot;, dev_file)
print(&quot;Test file:&quot;, test_file)
print(&quot;Raw file:&quot;, raw_file)
print(&quot;Char emb:&quot;, char_emb)
print(&quot;Bichar emb:&quot;, bichar_emb)
print(&quot;Gaz file:&quot;, gaz_file)
if status == &#39;train&#39;:
    print(&quot;Model saved to:&quot;, save_model_dir)
sys.stdout.flush()

if status == &#39;train&#39;:
    data = Data()
    data.HP_gpu = gpu
    data.HP_use_char = False
    data.HP_batch_size = 1
    data.use_bigram = False
    data.gaz_dropout = 0.5
    data.norm_gaz_emb = False
    data.HP_fix_gaz_emb = False
    data_initialization(data, gaz_file, train_file, dev_file, test_file)
    data.generate_instance_with_gaz(train_file, &#39;train&#39;)
    data.generate_instance_with_gaz(dev_file, &#39;dev&#39;)
    data.generate_instance_with_gaz(test_file, &#39;test&#39;)
    data.build_word_pretrain_emb(char_emb)
    data.build_biword_pretrain_emb(bichar_emb)
    data.build_gaz_pretrain_emb(gaz_file)
    train(data, save_model_dir, seg)
elif status == &#39;test&#39;:
    data = load_data_setting(dset_dir)
    data.generate_instance_with_gaz(dev_file, &#39;dev&#39;)
    load_model_decode(model_dir, data, &#39;dev&#39;, gpu, seg)
    data.generate_instance_with_gaz(test_file, &#39;test&#39;)
    load_model_decode(model_dir, data, &#39;test&#39;, gpu, seg)
elif status == &#39;decode&#39;:
    data = load_data_setting(dset_dir)
    data.generate_instance_with_gaz(raw_file, &#39;raw&#39;)
    decode_results = load_model_decode(model_dir, data, &#39;raw&#39;, gpu, seg)
    data.write_decoded_results(output_file, decode_results, &#39;raw&#39;)
else:
    print(&quot;Invalid argument! Please use valid arguments! (train/test/decode)&quot;)
</code></pre>
<p>‘’’</p>
]]></content>
      <categories>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Soft_Lexicon</title>
    <url>/2022/02/21/Soft-Lexicon/</url>
    <content><![CDATA[<p><img src="/2022/02/21/Soft-Lexicon/soft_lexicon.png" alt="soft_lexicon"></p>
<span id="more"></span>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1><p>import os<br>import gc<br>import sys<br>import time<br>import argparse<br>import random<br>import copy<br>import torch<br>import pickle<br>import numpy as np<br>import torch.autograd as autograd<br>import torch.optim as optim<br>from utils.data import Data<br>from utils.metric import get_ner_fmeasure<br>from model.gazlstm import GazLSTM as SeqModel</p>
<p>def data_initialization(data, gaz_file, train_file, dev_file, test_file):<br>    data.build_alphabet(train_file)<br>    data.build_alphabet(dev_file)<br>    data.build_alphabet(test_file)<br>    data.build_gaz_file(gaz_file)<br>    data.build_gaz_alphabet(train_file, count=True)<br>    data.build_gaz_alphabet(dev_file, count=True)<br>    data.build_gaz_alphabet(test_file, count=True)<br>    data.fix_alphabet()<br>    return data</p>
<p>def predict_check(pred_variable, gold_variable, mask_variable):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result, in numpy format<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””</p>
<pre><code>pred = pred_variable.cpu().data.numpy()
gold = gold_variable.cpu().data.numpy()
mask = mask_variable.cpu().data.numpy()
overlaped = (pred == gold)
right_token = np.sum(overlaped * mask)
total_token = mask.sum()

return right_token, total_token
</code></pre>
<p>def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet):<br>    “””<br>        input:<br>            pred_variable (batch_size, sent_len): pred tag result<br>            gold_variable (batch_size, sent_len): gold result variable<br>            mask_variable (batch_size, sent_len): mask variable<br>    “””<br>    batch_size = gold_variable.size(0)<br>    seq_len = gold_variable.size(1)<br>    mask = mask_variable.cpu().data.numpy()<br>    pred_tag = pred_variable.cpu().data.numpy()<br>    gold_tag = gold_variable.cpu().data.numpy()<br>    batch_size = mask.shape[0]<br>    pred_label = []<br>    gold_label = []<br>    for idx in range(batch_size):<br>        pred = [label_alphabet.get_instance(int(pred_tag[idx][idy])) for idy in range(seq_len) if mask[idx][idy] != 0]<br>        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]</p>
<pre><code>    assert (len(pred) == len(gold))
    pred_label.append(pred)
    gold_label.append(gold)

return pred_label, gold_label
</code></pre>
<p>def print_batchword(data, batch_word, n):<br>    with open(“labels/batchwords.txt”, “a”) as fp:<br>        for i in range(len(batch_word)):<br>            words = []<br>            for id in batch_word[i]:<br>                words.append(data.word_alphabet.get_instance(id))<br>            fp.write(str(words))</p>
<p>def save_data_setting(data, save_file):<br>    new_data = copy.deepcopy(data)<br>    # remove input instances<br>    new_data.train_texts = []<br>    new_data.dev_texts = []<br>    new_data.test_texts = []<br>    new_data.raw_texts = []</p>
<pre><code>new_data.train_Ids = []
new_data.dev_Ids = []
new_data.test_Ids = []
new_data.raw_Ids = []
# save data settings
with open(save_file, &#39;wb&#39;) as fp:
    pickle.dump(new_data, fp)
print(&quot;Data setting saved to file: &quot;, save_file)
</code></pre>
<p>def load_data_setting(save_file):<br>    with open(save_file, ‘rb’) as fp:<br>        data = pickle.load(fp)<br>    print(“Data setting loaded from file: “, save_file)<br>    data.show_data_summary()<br>    return data</p>
<p>def lr_decay(optimizer, epoch, decay_rate, init_lr):<br>    lr = init_lr * ((1 - decay_rate) ** epoch)<br>    print(“ Learning rate is setted as:”, lr)<br>    for param_group in optimizer.param_groups:<br>        param_group[‘lr’] = lr<br>    return optimizer</p>
<p>def set_seed(seed_num=1023):<br>    random.seed(seed_num)<br>    torch.manual_seed(seed_num)<br>    np.random.seed(seed_num)</p>
<p>def evaluate(data, model, name):<br>    if name == “train”:<br>        instances = data.train_Ids<br>    elif name == “dev”:<br>        instances = data.dev_Ids<br>    elif name == ‘test’:<br>        instances = data.test_Ids<br>    elif name == ‘raw’:<br>        instances = data.raw_Ids<br>    else:<br>        print(“Error: wrong evaluate name,”, name)<br>    pred_results = []<br>    gold_results = []<br>    # set model in eval model<br>    model.eval()<br>    batch_size = 1<br>    start_time = time.time()<br>    train_num = len(instances)<br>    total_batch = train_num // batch_size + 1<br>    gazes = []<br>    for batch_id in range(total_batch):<br>        with torch.no_grad():<br>            start = batch_id * batch_size<br>            end = (batch_id + 1) * batch_size<br>            if end &gt; train_num:<br>                end = train_num<br>            instance = instances[start:end]<br>            if not instance:<br>                continue<br>            gaz_list, batch_word, batch_biword, batch_wordlen, batch_label, layer_gaz, gaz_count, gaz_chars, gaz_mask, gazchar_mask, mask, batch_bert, bert_mask = batchify_with_label(<br>                instance, data.HP_gpu, data.HP_num_layer, True)<br>            tag_seq, gaz_match = model(gaz_list, batch_word, batch_biword, batch_wordlen, layer_gaz, gaz_count,<br>                                       gaz_chars, gaz_mask, gazchar_mask, mask, batch_bert, bert_mask)</p>
<pre><code>        gaz_list = [data.gaz_alphabet.get_instance(id) for batchlist in gaz_match if len(batchlist) &gt; 0 for id in
                    batchlist]
        gazes.append(gaz_list)

        if name == &quot;dev&quot;:
            pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet)
        else:
            pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet)
        pred_results += pred_label
        gold_results += gold_label
decode_time = time.time() - start_time
speed = len(instances) / decode_time
acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)
return speed, acc, p, r, f, pred_results, gazes
</code></pre>
<p>def get_text_input(self, caption):<br>    caption_tokens = self.tokenizer.tokenize(caption)<br>    caption_tokens = [‘[CLS]’] + caption_tokens + [‘[SEP]’]<br>    caption_ids = self.tokenizer.convert_tokens_to_ids(caption_tokens)<br>    if len(caption_ids) &gt;= self.max_seq_len:<br>        caption_ids = caption_ids[:self.max_seq_len]<br>    else:<br>        caption_ids = caption_ids + [0] * (self.max_seq_len - len(caption_ids))<br>    caption = torch.tensor(caption_ids)<br>    return caption</p>
<p>def batchify_with_label(input_batch_list, gpu, num_layer, volatile_flag=False):<br>    batch_size = len(input_batch_list)<br>    words = [sent[0] for sent in input_batch_list]<br>    biwords = [sent[1] for sent in input_batch_list]<br>    gazs = [sent[3] for sent in input_batch_list]<br>    labels = [sent[4] for sent in input_batch_list]<br>    layer_gazs = [sent[5] for sent in input_batch_list]<br>    gaz_count = [sent[6] for sent in input_batch_list]<br>    gaz_chars = [sent[7] for sent in input_batch_list]<br>    gaz_mask = [sent[8] for sent in input_batch_list]<br>    gazchar_mask = [sent[9] for sent in input_batch_list]<br>    # bert tokens<br>    bert_ids = [sent[10] for sent in input_batch_list]</p>
<pre><code>word_seq_lengths = torch.LongTensor(list(map(len, words)))
max_seq_len = word_seq_lengths.max()
word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()
biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()
label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()
mask = autograd.Variable(torch.zeros((batch_size, max_seq_len))).byte()
# bert seq tensor
bert_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len + 2))).long()
bert_mask = autograd.Variable(torch.zeros((batch_size, max_seq_len + 2))).long()

gaz_num = [len(layer_gazs[i][0][0]) for i in range(batch_size)]
max_gaz_num = max(gaz_num)
layer_gaz_tensor = torch.zeros(batch_size, max_seq_len, 4, max_gaz_num).long()
gaz_count_tensor = torch.zeros(batch_size, max_seq_len, 4, max_gaz_num).float()
gaz_len = [len(gaz_chars[i][0][0][0]) for i in range(batch_size)]
max_gaz_len = max(gaz_len)
gaz_chars_tensor = torch.zeros(batch_size, max_seq_len, 4, max_gaz_num, max_gaz_len).long()
gaz_mask_tensor = torch.ones(batch_size, max_seq_len, 4, max_gaz_num).byte()
gazchar_mask_tensor = torch.ones(batch_size, max_seq_len, 4, max_gaz_num, max_gaz_len).byte()

for b, (seq, bert_id, biseq, label, seqlen, layergaz, gazmask, gazcount, gazchar, gazchar_mask, gaznum,
        gazlen) in enumerate(
    zip(words, bert_ids, biwords, labels, word_seq_lengths, layer_gazs, gaz_mask, gaz_count, gaz_chars,
        gazchar_mask, gaz_num, gaz_len)):
    word_seq_tensor[b, :seqlen] = torch.LongTensor(seq)
    biword_seq_tensor[b, :seqlen] = torch.LongTensor(biseq)
    label_seq_tensor[b, :seqlen] = torch.LongTensor(label)
    layer_gaz_tensor[b, :seqlen, :, :gaznum] = torch.LongTensor(layergaz)
    mask[b, :seqlen] = torch.Tensor([1] * int(seqlen))
    bert_mask[b, :seqlen + 2] = torch.LongTensor([1] * int(seqlen + 2))
    gaz_mask_tensor[b, :seqlen, :, :gaznum] = torch.ByteTensor(gazmask)
    gaz_count_tensor[b, :seqlen, :, :gaznum] = torch.FloatTensor(gazcount)
    gaz_count_tensor[b, seqlen:] = 1
    gaz_chars_tensor[b, :seqlen, :, :gaznum, :gazlen] = torch.LongTensor(gazchar)
    gazchar_mask_tensor[b, :seqlen, :, :gaznum, :gazlen] = torch.ByteTensor(gazchar_mask)
    # bert
    bert_seq_tensor[b, :seqlen + 2] = torch.LongTensor(bert_id)

if gpu:
    word_seq_tensor = word_seq_tensor.cuda()
    biword_seq_tensor = biword_seq_tensor.cuda()
    word_seq_lengths = word_seq_lengths.cuda()
    label_seq_tensor = label_seq_tensor.cuda()
    layer_gaz_tensor = layer_gaz_tensor.cuda()
    gaz_chars_tensor = gaz_chars_tensor.cuda()
    gaz_mask_tensor = gaz_mask_tensor.cuda()
    gazchar_mask_tensor = gazchar_mask_tensor.cuda()
    gaz_count_tensor = gaz_count_tensor.cuda()
    mask = mask.cuda()
    bert_seq_tensor = bert_seq_tensor.cuda()
    bert_mask = bert_mask.cuda()

# print(bert_seq_tensor.type())
return gazs, word_seq_tensor, biword_seq_tensor, word_seq_lengths, label_seq_tensor, layer_gaz_tensor, gaz_count_tensor, gaz_chars_tensor, gaz_mask_tensor, gazchar_mask_tensor, mask, bert_seq_tensor, bert_mask
</code></pre>
<p>def train(data, save_model_dir, seg=True):<br>    print(“Training with {} model.”.format(data.model_type))<br>    # data.show_data_summary()<br>    model = SeqModel(data)<br>    print(“finish building model.”)</p>
<pre><code>parameters = filter(lambda p: p.requires_grad, model.parameters())
optimizer = optim.Adamax(parameters, lr=data.HP_lr)

best_dev = -1
best_dev_p = -1
best_dev_r = -1

best_test = -1
best_test_p = -1
best_test_r = -1

# start training
for idx in range(data.HP_iteration):
    epoch_start = time.time()
    temp_start = epoch_start
    print((&quot;Epoch: %s/%s&quot; % (idx, data.HP_iteration)))
    optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)
    instance_count = 0
    sample_loss = 0
    batch_loss = 0
    total_loss = 0
    right_token = 0
    whole_token = 0
    random.shuffle(data.train_Ids)
    # set model in train model
    model.train()
    model.zero_grad()
    batch_size = data.HP_batch_size
    batch_id = 0
    train_num = len(data.train_Ids)
    total_batch = train_num // batch_size + 1

    for batch_id in range(total_batch):
        start = batch_id * batch_size
        end = (batch_id + 1) * batch_size
        if end &gt; train_num:
            end = train_num
        instance = data.train_Ids[start:end]
        words = data.train_texts[start:end]
        if not instance:
            continue

        gaz_list, batch_word, batch_biword, batch_wordlen, batch_label, layer_gaz, gaz_count, gaz_chars, gaz_mask, gazchar_mask, mask, batch_bert, bert_mask = batchify_with_label(
            instance, data.HP_gpu, data.HP_num_layer)

        instance_count += 1
        loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, layer_gaz,
                                                      gaz_count, gaz_chars, gaz_mask, gazchar_mask, mask,
                                                      batch_label, batch_bert, bert_mask)

        right, whole = predict_check(tag_seq, batch_label, mask)
        right_token += right
        whole_token += whole
        sample_loss += loss.data
        total_loss += loss.data
        batch_loss += loss

        if end % 500 == 0:
            temp_time = time.time()
            temp_cost = temp_time - temp_start
            temp_start = temp_time
            print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; % (
                end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
            sys.stdout.flush()
            sample_loss = 0
        if end % data.HP_batch_size == 0:
            batch_loss.backward()
            optimizer.step()
            model.zero_grad()
            batch_loss = 0

    temp_time = time.time()
    temp_cost = temp_time - temp_start
    print((&quot;     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f&quot; % (
        end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token)))
    epoch_finish = time.time()
    epoch_cost = epoch_finish - epoch_start
    print((&quot;Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s&quot; % (
        idx, epoch_cost, train_num / epoch_cost, total_loss)))

    speed, acc, p, r, f, pred_labels, gazs = evaluate(data, model, &quot;dev&quot;)
    dev_finish = time.time()
    dev_cost = dev_finish - epoch_finish

    if seg:
        current_score = f
        print((&quot;Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
            dev_cost, speed, acc, p, r, f)))
    else:
        current_score = acc
        print((&quot;Dev: time: %.2fs speed: %.2fst/s; acc: %.4f&quot; % (dev_cost, speed, acc)))

    if current_score &gt; best_dev:
        if seg:
            print(&quot;Exceed previous best f score:&quot;, best_dev)

        else:
            print(&quot;Exceed previous best acc score:&quot;, best_dev)

        model_name = save_model_dir
        torch.save(model.state_dict(), model_name)
        # best_dev = current_score
        best_dev_p = p
        best_dev_r = r

    # ## decode test
    speed, acc, p, r, f, pred_labels, gazs = evaluate(data, model, &quot;test&quot;)
    test_finish = time.time()
    test_cost = test_finish - dev_finish
    if seg:
        current_test_score = f
        print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f&quot; % (
            test_cost, speed, acc, p, r, f)))
    else:
        current_test_score = acc
        print((&quot;Test: time: %.2fs, speed: %.2fst/s; acc: %.4f&quot; % (test_cost, speed, acc)))

    if current_score &gt; best_dev:
        best_dev = current_score
        best_test = current_test_score
        best_test_p = p
        best_test_r = r

    print(&quot;Best dev score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;&quot;.format(best_dev_p, best_dev_r, best_dev))
    print(&quot;Test score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;&quot;.format(best_test_p, best_test_r, best_test))
    gc.collect()

with open(data.result_file, &quot;a&quot;) as f:
    f.write(save_model_dir + &#39;\n&#39;)
    f.write(&quot;Best dev score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;\n&quot;.format(best_dev_p, best_dev_r, best_dev))
    f.write(&quot;Test score: p:&#123;&#125;, r:&#123;&#125;, f:&#123;&#125;\n\n&quot;.format(best_test_p, best_test_r, best_test))
    f.close()
</code></pre>
<p>def load_model_decode(model_dir, data, name, gpu, seg=True):<br>    data.HP_gpu = gpu<br>    print(“Load Model from file: “, model_dir)<br>    model = SeqModel(data)<br>    model.load_state_dict(torch.load(model_dir))<br>    print((“Decode %s data …” % (name)))<br>    start_time = time.time()<br>    speed, acc, p, r, f, pred_results, gazs = evaluate(data, model, name)<br>    end_time = time.time()<br>    time_cost = end_time - start_time<br>    if seg:<br>        print((“%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f” % (<br>            name, time_cost, speed, acc, p, r, f)))<br>    else:<br>        print((“%s: time:%.2fs, speed:%.2fst/s; acc: %.4f” % (name, time_cost, speed, acc)))</p>
<pre><code>return pred_results
</code></pre>
<p>def print_results(pred, modelname=””):<br>    toprint = []<br>    for sen in pred:<br>        sen = “ “.join(sen) + ‘\n’<br>        toprint.append(sen)<br>    with open(modelname + ‘_labels.txt’, ‘w’) as f:<br>        f.writelines(toprint)</p>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(‘–embedding’, help=’Embedding for words’, default=’None’)<br>    parser.add_argument(‘–status’, choices=[‘train’, ‘test’], help=’update algorithm’, default=’train’)<br>    parser.add_argument(‘–modelpath’, default=”save_model/“)<br>    parser.add_argument(‘–modelname’, default=”model”)<br>    parser.add_argument(‘–savedset’, help=’Dir of saved data setting’, default=”data/save.dset”)<br>    parser.add_argument(‘–train’, default=”data/ResumeNER/train.char.bmes”)<br>    parser.add_argument(‘–dev’, default=”data/ResumeNER/dev.char.bmes”)<br>    parser.add_argument(‘–test’, default=”data/ResumeNER/test.char.bmes”)<br>    parser.add_argument(‘–seg’, default=”True”)<br>    parser.add_argument(‘–extendalphabet’, default=”True”)<br>    parser.add_argument(‘–raw’)<br>    parser.add_argument(‘–output’)<br>    parser.add_argument(‘–seed’, default=1023, type=int)<br>    parser.add_argument(‘–labelcomment’, default=””)<br>    parser.add_argument(‘–resultfile’, default=”result/result.txt”)<br>    parser.add_argument(‘–num_iter’, default=100, type=int)<br>    parser.add_argument(‘–num_layer’, default=4, type=int)<br>    parser.add_argument(‘–lr’, type=float, default=0.0015)<br>    parser.add_argument(‘–batch_size’, type=int, default=1)<br>    parser.add_argument(‘–hidden_dim’, type=int, default=300)<br>    parser.add_argument(‘–model_type’, default=’lstm’)<br>    parser.add_argument(‘–drop’, type=float, default=0.5)</p>
<pre><code>parser.add_argument(&#39;--char_emb&#39;, help=&#39;&#39;, default=&quot;data/gigaword_chn.all.a2b.uni.ite50.vec&quot;)
parser.add_argument(&#39;--bichar_emb&#39;, help=&#39;&#39;, default=&quot;data/gigaword_chn.all.a2b.bi.ite50.vec&quot;)
parser.add_argument(&#39;--gaz_file&#39;, help=&#39;&#39;, default=&quot;data/ctb.50d.vec&quot;)

parser.add_argument(&#39;--use_biword&#39;, dest=&#39;use_biword&#39;, action=&#39;store_true&#39;, default=False)
# parser.set_defaults(use_biword=False)
parser.add_argument(&#39;--use_char&#39;, dest=&#39;use_char&#39;, action=&#39;store_true&#39;, default=False)
# parser.set_defaults(use_biword=False)
parser.add_argument(&#39;--use_count&#39;, action=&#39;store_true&#39;, default=True)
parser.add_argument(&#39;--use_bert&#39;, action=&#39;store_true&#39;, default=False)

args = parser.parse_args()

seed_num = args.seed
set_seed(seed_num)

train_file = args.train
dev_file = args.dev
test_file = args.test
raw_file = args.raw
# model_dir = args.loadmodel
output_file = args.output
if args.seg.lower() == &quot;true&quot;:
    seg = True
else:
    seg = False
status = args.status.lower()

save_model_dir = args.modelpath + args.modelname
save_data_name = args.savedset
gpu = torch.cuda.is_available()

char_emb = args.char_emb
bichar_emb = args.bichar_emb
gaz_file = args.gaz_file
sys.stdout.flush()

if status == &#39;train&#39;:
    if os.path.exists(save_data_name):
        print(&#39;Loading processed train data&#39;)
        with open(save_data_name, &#39;rb&#39;) as fp:
            data = pickle.load(fp)
        if data.HP_gpu and not gpu:
            print(&quot;GPU is not available. \nPlease use other dir to save data setting!&quot;)
        data.HP_num_layer = args.num_layer
        data.HP_batch_size = args.batch_size
        data.HP_iteration = args.num_iter
        data.label_comment = args.labelcomment
        data.result_file = args.resultfile
        data.HP_lr = args.lr
        data.use_bigram = args.use_biword
        data.HP_use_char = args.use_char
        data.HP_hidden_dim = args.hidden_dim
        data.HP_dropout = args.drop
        data.HP_use_count = args.use_count
        data.model_type = args.model_type
        data.use_bert = args.use_bert
    else:
        data = Data()
        data.HP_gpu = gpu
        data.HP_use_char = args.use_char
        data.HP_batch_size = args.batch_size
        data.HP_num_layer = args.num_layer
        data.HP_iteration = args.num_iter
        data.use_bigram = args.use_biword
        data.HP_dropout = args.drop
        data.norm_gaz_emb = False
        data.HP_fix_gaz_emb = False
        data.label_comment = args.labelcomment
        data.result_file = args.resultfile
        data.HP_lr = args.lr
        data.HP_hidden_dim = args.hidden_dim
        data.HP_use_count = args.use_count
        data.model_type = args.model_type
        data.use_bert = args.use_bert
        data_initialization(data, gaz_file, train_file, dev_file, test_file)
        data.generate_instance_with_gaz(train_file, &#39;train&#39;)
        data.generate_instance_with_gaz(dev_file, &#39;dev&#39;)
        data.generate_instance_with_gaz(test_file, &#39;test&#39;)
        data.build_word_pretrain_emb(char_emb)
        data.build_biword_pretrain_emb(bichar_emb)
        data.build_gaz_pretrain_emb(gaz_file)

        print(&#39;Dumping data&#39;)
        with open(save_data_name, &#39;wb&#39;) as f:
            pickle.dump(data, f)
        set_seed(seed_num)
    print(&#39;data.use_biword=&#39;, data.use_bigram)
    train(data, save_model_dir, seg)
elif status == &#39;test&#39;:
    print(&#39;Loading processed test data&#39;)
    with open(save_data_name, &#39;rb&#39;) as fp:
        data = pickle.load(fp)
    data.HP_num_layer = args.num_layer
    data.HP_iteration = args.num_iter
    data.label_comment = args.labelcomment
    data.result_file = args.resultfile
    # data.HP_use_gaz = args.use_gaz
    data.HP_lr = args.lr
    data.use_bigram = args.use_biword
    data.HP_use_char = args.use_char
    data.model_type = args.model_type
    data.HP_hidden_dim = args.hidden_dim
    data.HP_use_count = args.use_count
    data.generate_instance_with_gaz(test_file, &#39;test&#39;)
    load_model_decode(save_model_dir, data, &#39;test&#39;, gpu, seg)

else:
    print(&quot;Invalid argument! Please use valid arguments! (train/test/decode)&quot;)
</code></pre>
]]></content>
      <categories>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
